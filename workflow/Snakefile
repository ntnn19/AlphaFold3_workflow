import string
from pathlib import Path
import pandas as pd
import os
import yaml
from numpy.random import sample

localrules: PREPROCESSING , MERGE_MONO_AND_MULTI_JSON, CREATE_AF3_INFERENCE_JOBS_SPEEDY_AF3_PIPELINE, AGG_AF3_INFERENCE_JOBS, SPLIT_INFERENCE_JOB_LIST

scattergather:
    split=config.get("n_splits",1),
    split_ost=config.get("n_splits_ost",1)

SAMPLE_SHEET_SCHEMAS = {
    "raw_data": [
        "job_name", "type", "id", "sequence", "modifications", "ccd_codes",
        "smiles", "msa_option", "unpaired_msa", "paired_msa", "templates",
        "model_seeds", "bonded_atom_pairs", "user_ccd"
    ],
    "data_pipeline_ready": ["sample_id", "file"],
    "inference_ready": ["sample_id", "file"],
    "merge_ready_samples": ["sample_id", "multimer_file", "monomer_chain_id", "monomer_file"],
}

def as_bool(val):
    if isinstance(val, bool):
        return val
    if isinstance(val, (int, float)):
        return bool(val)
    if isinstance(val, str):
        return val.strip().lower() in {"1", "true", "yes", "y", "on"}
    return False

def bool_flag(name, value):
    return f"--{name}" if as_bool(value) else ""



def as_bool(val):
    if isinstance(val, bool):
        return val
    if isinstance(val, (int, float)):
        return bool(val)
    if isinstance(val, str):
        return val.strip().lower() in {"1", "true", "yes", "y", "on"}
    return False

def bool_flag(name, value):
    return f"--{name}" if as_bool(value) else ""

def load_sample_sheet(sheet_key):
    """Safely load a sample sheet, returns (path, dataframe)"""
    path = config.get("sample_sheets",{}).get(sheet_key)

    columns = SAMPLE_SHEET_SCHEMAS.get(sheet_key,[])

    if not path:
        return None, pd.DataFrame(columns=columns)

    if not Path(path).exists():
        return path, pd.DataFrame(columns=columns)

    try:
        df = pd.read_csv(path,sep="\t")
        return path, df
    except Exception as e:
        return path, pd.DataFrame(columns=columns)




RAW_DATA_PATH, RAW_DATA_DF = load_sample_sheet("raw_data")
DATA_PIPELINE_READY_PATH, DATA_PIPELINE_READY_DF = load_sample_sheet("data_pipeline_ready")
INFERENCE_READY_PATH, INFERENCE_READY_DF = load_sample_sheet("inference_ready")
MERGE_READY_PATH, MERGE_READY_DF = load_sample_sheet("merge_ready")
OUTPUT_DIR = config.get("output_dir","results")

if not DATA_PIPELINE_READY_DF.empty and MERGE_READY_DF.empty: # Create dummy merging table
    MERGE_READY_DF = DATA_PIPELINE_READY_DF.copy()
    MERGE_READY_DF = MERGE_READY_DF.rename(columns={"file": "multimer_file"})
    MERGE_READY_DF["monomer_file"] = MERGE_READY_DF["multimer_file"].apply(lambda x: os.path.join(OUTPUT_DIR, "rule_AF3_DATA_PIPELINE", Path(x).stem, Path(x).stem + "_data.json"))
    MERGE_READY_DF["monomer_chain_id"] = "A"
    DATA_PIPELINE_OUTPUTS = MERGE_READY_DF["monomer_file"].tolist() 

MODE = config.get("mode","custom")
EXCLUSIVE_LOCK = config.get("exclusive_lock",False)
OST_CONTAINER = config.get("ost_container")
GROUND_TRUTH =config.get("ground_truth")
TASK = config.get("task", "")
PREDICT_INDIVIDUAL_COMPONENTS = bool_flag("predict_individual_components",config.get("predict_individual_components",False))
N_SEEDS = config.get("n_seeds") # See TODO
N_SAMPLES = config.get("n_samples")
MSA_OPTION = config.get("msa_option","auto")
SPLIT_TOTAL = workflow._scatter["split"]
SPLIT_OST = workflow._scatter["split_ost"]



WORKFLOW_DIR = os.path.dirname(os.path.abspath(workflow.snakefile))


AF3_CONTAINER = config["af3_flags"]["--af3_container"]
EXTRA_AF3_FLAGS = config.get('alphafold3_flags', {}).get("--extra_af3_flags", '')



has_seeds = (
    "model_seeds" in RAW_DATA_DF.columns
    and RAW_DATA_DF["model_seeds"].notna().any()
    and RAW_DATA_DF["model_seeds"].astype(str).str.strip().ne("").any()
)


if N_SEEDS:
    n_seeds = f"--n-seeds={N_SEEDS}"   # explicit override wins
elif has_seeds:
    n_seeds = ""                      # sample sheet drives it, don't pass the flag
else:
    n_seeds = "--n-seeds=1"            # fallback default




def sanitised_name(name):
    """Returns sanitised version of the name that can be used as a filename."""
    lower_spaceless_name = name.lower().replace(' ', '_')
    allowed_chars = set(string.ascii_lowercase + string.digits + '_-.')
    return ''.join(l for l in lower_spaceless_name if l in allowed_chars)


def get_preprocessing_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES, = glob_wildcards(os.path.join(PREPROCESSING_DIR, "{i}.json"))
    return list(expand(os.path.join(PREPROCESSING_DIR,"{i}.json"),i=JOB_NAMES))

def get_individual_jobs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES, = glob_wildcards(os.path.join(PREPROCESSING_DIR, "{i}.json"))
    return list(expand(os.path.join(OUTPUT_DIR,"CREATE_AF3_INFERENCE_JOBS","{i}_af3_inference_job.txt"),i=JOB_NAMES))

def get_data_pipeline_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"{i}.json"))
    return list(expand(os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","{i}/{i}_data.json"),i=JOB_NAMES))

def get_multimeric_json_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES_MULTIMERS, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"multimers","{multi}.json"))
    return list(expand(os.path.join(PREPROCESSING_DIR,"multimers","{multi}.json"),multi=JOB_NAMES_MULTIMERS)) + list(expand(os.path.join(OUTPUT_DIR,"rule_MERGE_MONOMERS_TO_MULTIMERS","{multi}_data.json"), multi=JOB_NAMES_MULTIMERS))

def get_monomeric_json_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES_MONOMERS, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"monomers","{mono}.json"))
    return list(expand(os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","{mono}/{mono}_data.json"),mono=JOB_NAMES_MONOMERS))

def get_multi_to_monomeric_dict(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    map_df = pd.read_csv(os.path.join(PREPROCESSING_DIR,"metadata","inference_to_data_pipeline_map.tsv"),sep="\t")
    return map_df

def get_multimeric_json_with_msas(wildcards):
    internal = []
    external = []
    if not DATA_PIPELINE_READY_DF.empty:
        if EXCLUSIVE_LOCK:
            external.append(DATA_PIPELINE_READY_DF["file"].apply(lambda x: f"{OUTPUT_DIR}/rule_CREATE_AF3_INFERENCE_JOBS/" + f"{Path(x).stem}_af3_inference_job.txt").unique().tolist())

        else:
            external.append(DATA_PIPELINE_READY_DF[
                "file"].apply(lambda x: f"{OUTPUT_DIR}/rule_AF3_INFERENCE/" + f"{Path(x).stem}/{Path(x).stem}_model.cif").unique().tolist())


    if not MERGE_READY_DF.empty:
        if EXCLUSIVE_LOCK:
            external.append(MERGE_READY_DF["multimer_file"].apply(lambda x: f"{OUTPUT_DIR}/rule_CREATE_AF3_INFERENCE_JOBS/" + f"{Path(x).stem}_af3_inference_job.txt").unique().tolist())

        else:
            external.append(MERGE_READY_DF[
                "multimer_file"].apply(lambda x: f"{OUTPUT_DIR}/rule_AF3_INFERENCE/" + f"{Path(x).stem}/{Path(x).stem}_model.cif").unique().tolist())

    if not INFERENCE_READY_DF.empty:
        if EXCLUSIVE_LOCK:
            external.append(INFERENCE_READY_DF["file"].apply(lambda x: f"{OUTPUT_DIR}/rule_CREATE_AF3_INFERENCE_JOBS/" + f"{Path(x).stem}_af3_inference_job.txt").unique().tolist())

        else:
            external.append(INFERENCE_READY_DF[
                "file"].apply(lambda x: f"{OUTPUT_DIR}/rule_AF3_INFERENCE/" + f"{Path(x).stem}/{Path(x).stem}_model.cif").unique().tolist())

    if not RAW_DATA_DF.empty:
        PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
        JOB_NAMES_MULTIMERS, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"multimers","{multi}.json"))
        if EXCLUSIVE_LOCK:
            internal.append(list(expand(os.path.join(OUTPUT_DIR,"rule_CREATE_AF3_INFERENCE_JOBS","{multi}_af3_inference_job.txt"),multi=JOB_NAMES_MULTIMERS)))
        else:
            internal.append(list(expand(os.path.join(OUTPUT_DIR,"rule_AF3_INFERENCE","{multi}","{multi}_model.cif"),multi=JOB_NAMES_MULTIMERS)))

    if internal and external:
        return flatten(internal) + flatten(external)
    if internal and not external:
        return flatten(internal)
    if not internal and external:
        return flatten(external)
    if not internal and not external:
        return []

def get_collect_predictions(wildcards):
    GET_DONE_OUTPUTS_DIR = checkpoints.GET_DONE_OUTPUTS.get(**wildcards).output[0]
    JOB_NAMES_MULTIMERS, SEEDS_ = glob_wildcards(os.path.join(GET_DONE_OUTPUTS_DIR,"{multi}_seed-{seed}.done.txt"))
    files = list(expand(os.path.join(OUTPUT_DIR,"rule_CREATE_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","{multi}_seed-{seed}_sample-{sample}_job.txt"), multi=set(JOB_NAMES_MULTIMERS),seed=list(set(SEEDS_)), sample=[0,1,2,3,4]))
    return files

def get_af3_flag_value(flag, default_value):
    return config.get('alphafold3_flags', {}).get(flag, default_value)


def get_merge_inputs(wildcards):
    """
    Get inputs for merging. Check user-provided MERGE_READY_DF first,
    otherwise use checkpoint-generated mapping.

    Returns dict with:
    - multimer_template: the multimer JSON file
    - monomer_files: list of processed monomer files (one per chain)
    """

    if not MERGE_READY_DF.empty and wildcards.multi in MERGE_READY_DF['sample_id'].values:
        print("HERE")
        multimer_rows = MERGE_READY_DF[MERGE_READY_DF['sample_id'] == wildcards.multi]

        multimer_template = multimer_rows.iloc[0]['multimer_file']

        monomer_files = multimer_rows['monomer_file'].tolist()

        return {
            'multimer_template': multimer_template,
            'monomer_files': monomer_files
        }
    checkpoint_output = os.path.join(checkpoints.PREPROCESSING.get(**wildcards).output[0],"metadata","inference_to_data_pipeline_map.tsv")
    print("NO, HERE")
    mapping = pd.read_csv(checkpoint_output,sep="\t")

    multimer_rows = mapping[mapping['multimer_file'].str.contains(wildcards.multi)]
    monomers = multimer_rows['monomer_file'].tolist()
    print("wildcards.multi", wildcards.multi)
    print("multimer rows", multimer_rows)
    print("monomer rows", monomers)
    multimer_template = os.path.join(
        OUTPUT_DIR,
        "rule_PREPROCESSING",
        "multimers",
        f"{wildcards.multi}.json"
    )

 #   all_monomers = get_monomeric_json_outputs(wildcards)
 #   print(all_monomers)
 #   exit()
#    for mono in monomers:
 #       mono_file = os.path.join(
 #           OUTPUT_DIR,
 #           "rule_AF3_DATA_PIPELINE",
 #           mono,
 #           f"{mono}_data.json"
 #       )
        
#        monomer_files.append(mono_file)
    return {
        'multimer_template': multimer_template,
        'monomer_files': monomers
    }






if "model_seeds" in RAW_DATA_DF.columns:
    RAW_DATA_DF["model_seeds"] = RAW_DATA_DF["model_seeds"] = RAW_DATA_DF.model_seeds.astype(str)







rule all:
    input:
        os.path.join(OUTPUT_DIR,"rule_AGG_OST_REPORTS","ost_report.csv") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
        expand(os.path.join(OUTPUT_DIR,"rule_OST_COMAPRE_LIGAND_STRUCTURES","done_flags",f"job_{{s}}-of-{SPLIT_OST}.done.txt"),s=list(range(1,SPLIT_OST+1))) if MODE == "virtual-drug-screen" and TASK=="ost" else [],
        expand(os.path.join(OUTPUT_DIR,"rule_AF3_INFERENCE","done_flags",f"af3_inference_jobs_{{s}}-of-{SPLIT_TOTAL}.done.txt"),s=list(range(1,SPLIT_TOTAL+1))) if EXCLUSIVE_LOCK else get_multimeric_json_with_msas,
        DATA_PIPELINE_OUTPUTS if not DATA_PIPELINE_READY_DF.empty else [] #  and MERGE_READY_DF.empty else []


checkpoint PREPROCESSING:
    input:
        RAW_DATA_PATH if not RAW_DATA_DF.empty else [],
    output:
        directory(os.path.join(OUTPUT_DIR,"rule_PREPROCESSING")) if not RAW_DATA_DF.empty else []
    params:
        msa_option = MSA_OPTION, # implement this as a global option: msa-free/auto/other
        mode = MODE,
        n_seeds = n_seeds,
        n_samples = f"--n-samples={N_SAMPLES}" if N_SAMPLES else "",
        predict_individual_components = PREDICT_INDIVIDUAL_COMPONENTS
    shell:
        """
        python {WORKFLOW_DIR}/scripts/preprocessing.py \
        {input[0]} \
        {OUTPUT_DIR} \
        --mode={params.mode} \
        {params.predict_individual_components} {params.n_seeds} {params.n_samples} 
        """


rule AF3_DATA_SPEEDY_PIPELINE:
    input:
        data = branch(
            lookup(query="sample_id == '{mono}'",within=DATA_PIPELINE_READY_DF ,cols="file"),
            then=lookup(query="sample_id == '{mono}'",within=DATA_PIPELINE_READY_DF ,cols="file"),
            otherwise=os.path.join(OUTPUT_DIR,"rule_PREPROCESSING","monomers","{mono}.json")
        ),
    params:
        mode = MODE,
        extra_af3_flags = EXTRA_AF3_FLAGS
    output:
        data_pipeline_monomers=os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","{mono}","{mono}_data.json") if MODE in [
            "custom","stoichio-screen","all-vs-all", "pulldown","virtual-drug-screen"] else [],
    container:
        AF3_CONTAINER
    shell:
        """
        python /app/alphafold/run_alphafold.py --json_path={input.data} \
        --model_dir=/root/models \
        --output_dir=/root/af_output/rule_AF3_DATA_PIPELINE \
        --db_dir=/root/public_databases \
        --run_data_pipeline=true \
        --run_inference=false \
        {params.extra_af3_flags} 
        """

rule MERGE_MONO_AND_MULTI_JSON:
    input:
        unpack(get_merge_inputs)
    output:
        os.path.join(OUTPUT_DIR,"rule_MERGE_MONOMERS_TO_MULTIMERS","{multi}_data.json") if MODE in ["custom","all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"] else [],
    shell:
        """
        python {WORKFLOW_DIR}/scripts/merge_mono_and_multi_jsons.py {input} {output}
        """



rule CREATE_AF3_INFERENCE_JOBS_SPEEDY_AF3_PIPELINE:
    input:
        data = branch(
            lookup(query="sample_id == '{multi}'",within=INFERENCE_READY_DF,cols="file"),
            then=lookup(query="sample_id == '{multi}'",within=INFERENCE_READY_DF,cols="file"),
            otherwise=os.path.join(OUTPUT_DIR,"rule_MERGE_MONOMERS_TO_MULTIMERS","{multi}_data.json")
        ) if EXCLUSIVE_LOCK else [],
    params:
        mode = MODE,
        extra_af3_flags = EXTRA_AF3_FLAGS
    output:
        multimers = os.path.join(OUTPUT_DIR,"rule_CREATE_AF3_INFERENCE_JOBS","{multi}_af3_inference_job.txt") if MODE in ["custom","all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"]  else []
    shell:
        """
        echo 'python /app/alphafold/run_alphafold.py --json_path={input.data} \
        --model_dir=/root/models \
        --output_dir=/root/af_output/rule_AF3_INFERENCE \
        --db_dir=/root/public_databases \
        --run_data_pipeline=false \
        --run_inference=true \
        {params.extra_af3_flags}' > {output}
        """


rule AGG_AF3_INFERENCE_JOBS:
    input:
        merged_multimer = get_multimeric_json_with_msas if EXCLUSIVE_LOCK else []
    params:
        mode = MODE
    output:
        job_list = os.path.join(OUTPUT_DIR,"rule_AGG_AF3_INFERENCE_JOBS","af3_inference_jobs.txt")
    shell:
        """
        find {OUTPUT_DIR}/rule_CREATE_AF3_INFERENCE_JOBS -type f -name '*.txt' -print0 | tar --null -T - -cf - | tar -xOf - > {output.job_list}
        """

rule SPLIT_INFERENCE_JOB_LIST:
    input:
        inference_job_list = ancient(os.path.join(OUTPUT_DIR,"rule_AGG_AF3_INFERENCE_JOBS","af3_inference_jobs.txt")) if
        EXCLUSIVE_LOCK else []
    params:
        split_total = SPLIT_TOTAL
    output:
        sub_job_list = scatter.split(os.path.join(OUTPUT_DIR,"rule_SPLIT_INFERENCE_JOB_LIST","af3_inference_jobs_{scatteritem}.txt"))
    run:
        import os
        lines = open(input.inference_job_list).readlines()
        n_lines = len(lines)
        n_splits = params.split_total

        chunk_size = n_lines // n_splits
        remainder = n_lines % n_splits

        start = 0
        for i in range(n_splits):
            end = start + chunk_size + (1 if i < remainder else 0)

            with open(output.sub_job_list[i], "w") as f:
                f.writelines(lines[start:end])

            start = end


rule AF3_INFERENCE:
    input:
        data = ancient(os.path.join(OUTPUT_DIR,"rule_SPLIT_INFERENCE_JOB_LIST","af3_inference_jobs_{scatteritem}.txt")) if
        EXCLUSIVE_LOCK else branch(
            lookup(query="sample_id == '{multi}'",within=INFERENCE_READY_DF,cols="file"),
            then=lookup(query="sample_id == '{multi}'",within=INFERENCE_READY_DF,cols="file"),
            otherwise=os.path.join(OUTPUT_DIR,"rule_MERGE_MONOMERS_TO_MULTIMERS","{multi}_data.json")
        ),
    output:
        touch(os.path.join(OUTPUT_DIR,"rule_AF3_INFERENCE","done_flags","af3_inference_jobs_{scatteritem}.done.txt"))  if
        EXCLUSIVE_LOCK else os.path.join(OUTPUT_DIR,"rule_AF3_INFERENCE","{multi}","{multi}_model.cif"),
    params:
        extra_af3_flags = EXTRA_AF3_FLAGS,
        exclusive_lock = "true" if EXCLUSIVE_LOCK else "false"
    container:
        AF3_CONTAINER
    shell:
        """
        CC=$(nvidia-smi --query-gpu=compute_cap --format=csv,noheader,nounits | head -n 1 | cut -d'.' -f1)
        if [[ "$CC" -ge 8 ]]; then
            FLASH_ARG=""
        else
            export XLA_FLAGS="--xla_disable_hlo_passes=custom-kernel-fusion-rewriter"
            FLASH_ARG="--flash_attention_implementation=xla"
        fi
        if [  "{params.exclusive_lock}" == "true" ]; then
            echo "CC = ${{CC}}"
            echo "CC = ${{CC}}"
            echo "XLA_FLAGS = ${{XLA_FLAGS}}"
            echo "FLASH ARG = ${{FLASH_ARG}}"
            JOB_LIST={input.data}
            sed -Ei "s|/app/alphafold/run_alphafold.py\\b|/app/alphafold/run_alphafold.py ${{FLASH_ARG}}|" $JOB_LIST
            NUM_GPUS=$(nvidia-smi -L | wc -l)
            echo "Number of GPUs = ${{NUM_GPUS}}"
            < $JOB_LIST parallel -j $NUM_GPUS 'eval CUDA_VISIBLE_DEVICES=$(({{%}} - 1)) {{}}'
        else
            python /app/alphafold/run_alphafold.py $FLASH_ARG --json_path={input.data} \
            --model_dir=/root/models \
            --output_dir=/root/af_output/rule_AF3_INFERENCE \
            --db_dir=/root/public_databases \
            --run_data_pipeline=false \
            --run_inference=true \
            {params.extra_af3_flags} 
        fi
        """

checkpoint GET_DONE_OUTPUTS:
    input:
        expand(os.path.join(OUTPUT_DIR,"rule_AF3_INFERENCE","done_flags",f"af3_inference_jobs_{{s}}-of-{SPLIT_TOTAL}.done.txt"),s=list(range(1,SPLIT_TOTAL+1))),
    output:
        directory(os.path.join(OUTPUT_DIR,"rule_GET_DONE_OUTPUTS"))
    shell:
        """
        mkdir -p {output}
        for i in {{1..{SPLIT_TOTAL}}}; do
            eval \"$(grep -- '--json_path=' {OUTPUT_DIR}/rule_SPLIT_INFERENCE_JOB_LIST/af3_inference_jobs_${{i}}-of-{SPLIT_TOTAL}.txt  | sed -E 's/.*--json_path=\\/.*\\/([^/]+)_data\\.json.*/touch {OUTPUT_DIR}\\/rule_GET_DONE_OUTPUTS\\/\\1.done.txt/')\";
        done
        """

rule COLLECT_AF3_PREDICTIONS:
    input:
        done_files = os.path.join(OUTPUT_DIR,"rule_GET_DONE_OUTPUTS","{multi}_seed-{seed}.done.txt"),
        job = os.path.join(OUTPUT_DIR,"rule_CREATE_AF3_INFERENCE_JOBS","{multi}_seed-{seed}_af3_inference_job.txt") if MODE in ["all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"]  else []
    output:
        os.path.join(OUTPUT_DIR,"COLLECT_AF3_PREDICTIONS","{multi}_seed-{seed}_sample-{sample}_model.cif")
    shell:
        """
        python {WORKFLOW_DIR}/scripts/collect_predictions.py --source-dir {OUTPUT_DIR}/rule_AF3_INFERENCE/{wildcards.multi}_seed-{wildcards.seed} --job-list {input.job} --output-dir {OUTPUT_DIR}/rule_COLLECT_AF3_PREDICTIONS
        """

rule CREATE_OST_COMAPRE_LIGAND_STRUCTURES_JOBS:
    input:
        prediction = os.path.join(OUTPUT_DIR,"COLLECT_AF3_PREDICTIONS","{multi}_seed-{seed}_sample-{sample}_model.cif") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
        ground_truth_cif=lambda wildcards: get_ground_truth_files(wildcards)["cif"] if MODE == "virtual-drug-screen" and TASK=="ost" else [],
        ground_truth_sdf=lambda wildcards: get_ground_truth_files(wildcards)["sdf"] if MODE == "virtual-drug-screen" and TASK=="ost" else []
    output:
        os.path.join(OUTPUT_DIR,"rule_CREATE_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","{multi}_seed-{seed}_sample-{sample}_job.txt") if MODE == "virtual-drug-screen" and TASK=="ost" else []
    shell:
        """    
        json_path={OUTPUT_DIR}/rule_OST_COMAPRE_LIGAND_STRUCTURES/{wildcards.multi}_seed-{wildcards.seed}_sample-{wildcards.sample}_score.json
        echo "ost compare-ligand-structures \
        -m {input.prediction} \
        -rl {input.ground_truth_sdf} \
        -r {input.ground_truth_cif} \
        -o $json_path \
        --lddt-pli --rmsd --lddt-pli-add-mdl-contacts -ft" > {output}
        """

rule AGG_OST_COMAPRE_LIGAND_STRUCTURES_JOBS:
    input:
        get_collect_predictions,
    output:
        os.path.join(OUTPUT_DIR,"rule_AGG_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","jobs.txt")
    shell:
        """
        cat {input} > {output}
        """

rule SPLIT_OST_COMAPRE_LIGAND_STRUCTURES_JOBS:
    input:
         rules.AGG_OST_COMAPRE_LIGAND_STRUCTURES_JOBS.output if MODE == "virtual-drug-screen" and TASK=="ost" else [],
    params:
         split_ost = SPLIT_OST
    output:
         sub_job_list = scatter.split_ost(os.path.join(OUTPUT_DIR,"rule_SPLIT_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","job_{scatteritem}.txt"))
    run:
        import os
        lines = open(input[0]).readlines()
        [open(output.sub_job_list[i], "w").writelines(lines[i*len(lines)//params.split_ost:(i+1)*len(lines)//params.split_ost + (1 if i == 3 else 0)])
         for i in range(params.split_ost)]


rule OST_COMAPRE_LIGAND_STRUCTURES:
    input:
         sub_job_list = os.path.join(OUTPUT_DIR,"rule_SPLIT_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","job_{scatteritem}.txt") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
    output:
         touch(os.path.join(OUTPUT_DIR,"rule_OST_COMAPRE_LIGAND_STRUCTURES","done_flags","job_{scatteritem}.done.txt")),
    shell:
         """
         bash {WORKFLOW_DIR}/scripts/parallel_cpu.sh {input}
         """

rule CREATE_OST_REPORT:
    input:
         sub_job_list = os.path.join(OUTPUT_DIR,"rule_SPLIT_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","job_{scatteritem}.txt") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
         done_file = os.path.join(OUTPUT_DIR,"rule_OST_COMAPRE_LIGAND_STRUCTURES","done_flags","job_{scatteritem}.done.txt")
    output:
         os.path.join(OUTPUT_DIR,"CREATE_OST_REPORT","{scatteritem}_ost_report.csv")
    shell:
         """
         > {wildcards.scatteritem}.commands.sh
         grep -oP '(?<=-o )\\S+' {input.sub_job_list} | while read -r path; do echo "python {WORKFLOW_DIR}/scripts/af3_analysis.py.bak $path {RUNS_N_POSES_JSON_INPUTS} {OUTPUT_DIR}/AF3_INFERENCE {OUTPUT_DIR}/OST_COMAPRE_LIGAND_STRUCTURES {RUNS_N_POSES_ANNOTATIONS} >> {output}" >> {wildcards.scatteritem}.commands.sh; done
         chmod +x {wildcards.scatteritem}.commands.sh
         bash {WORKFLOW_DIR}/scripts/parallel_cpu.sh {wildcards.scatteritem}.commands.sh
         rm {wildcards.scatteritem}.commands.sh
         """

rule AGG_OST_REPORTS:
    input:
         expand(os.path.join(OUTPUT_DIR,"CREATE_OST_REPORT",f"{{s}}-of-{SPLIT_OST}_ost_report.csv"),s=list(range(1,SPLIT_OST+1))) if MODE == "virtual-drug-screen" and TASK=="ost" else [],
    output:
         os.path.join(OUTPUT_DIR,"rule_AGG_OST_REPORTS","ost_report.csv")
    shell:
        """
        > {output}
        echo "target method seed sample ranking_score prot_lig_chain_iptm_average_lddt_pli prot_lig_chain_iptm_min_lddt_pli prot_lig_chain_iptm_max_lddt_pli lig_prot_chain_iptm_average_lddt_pli lig_prot_chain_iptm_min_lddt_pli lig_prot_chain_iptm_max_lddt_pli lddt_pli model_ligand_chain_lddt_pli model_ligand_ccd_code model_ligand_smiles ligand_ccd_code_x prot_lig_chain_iptm_average_rmsd prot_lig_chain_iptm_min_rmsd prot_lig_chain_iptm_max_rmsd lig_prot_chain_iptm_average_rmsd lig_prot_chain_iptm_min_rmsd lig_prot_chain_iptm_max_rmsd rmsd lddt_lp bb_rmsd model_ligand_chain_rmsd ligand_ccd_code_y ligand_instance_chain ligand_is_proper" > {output}
        cat {input} >> {output}
        """

local_rule_options = {'run_data_pipeline_locally' : 'AF3_DATA_SPEEDY_PIPELINE', 'run_inference_locally' : 'AF3_INFERENCE'}
_localrules = list(workflow._localrules) # get the local rules so far
for option in local_rule_options:
    if config.get(option, False):
        _localrules.append(local_rule_options[option])


workflow._localrules = set(_localrules) # set the updated local rules
