import os
localrules: CREATE_AF3_INFERENCE_JOBS, AGG_AF3_INFERENCE_JOBS, SPLIT_INFERENCE_JOB_LIST
scattergather:
    split=config.get("n_splits",1)
SPLIT_TOTAL = workflow._scatter["split"]

from scripts import create_tasks_from_dataframe

WORKFLOW_DIR = workflow.basedir
INPUT_DF = config["input_csv"]
OUTPUT_DIR = config["output_dir"]
MODE = config.get("mode","default")
MSA_OPTION = config.get("msa_option","auto")
AF3_CONTAINER = config["af3_flags"]["--af3_container"]
def get_af3_flag_value(flag, default_value):
    return config.get('alphafold3_flags', {}).get(flag, default_value)

# Example usage: Getting flag values from YAML with fallbacks

BUCKETS = get_af3_flag_value('--buckets', '256,512,768,1024,1280,1536,2048,2560,3072,3584,4096,4608,5120')
CONFORMER_MAX_ITERATIONS = get_af3_flag_value('--conformer_max_iterations', 100)
FLASH_ATTENTION_IMPLEMENTATION = get_af3_flag_value('--flash_attention_implementation', 'triton')
GPU_DEVICE = get_af3_flag_value('--gpu_device', 0)
HMMALIGN_BINARY_PATH = get_af3_flag_value('--hmmalign_binary_path', '/hmmer/bin/hmmalign')
HMMBUILD_BINARY_PATH = get_af3_flag_value('--hmmbuild_binary_path', '/hmmer/bin/hmmbuild')
HMMSEARCH_BINARY_PATH = get_af3_flag_value('--hmmsearch_binary_path', '/hmmer/bin/hmmsearch')
JACKHMMER_BINARY_PATH = get_af3_flag_value('--jackhmmer_binary_path', '/hmmer/bin/jackhmmer')
JACKHMMER_N_CPU = get_af3_flag_value('--jackhmmer_n_cpu', 8)
JAX_COMPILATION_CACHE_DIR = get_af3_flag_value('--jax_compilation_cache_dir', '/path/to/cache')
MAX_TEMPLATE_DATE = get_af3_flag_value('--max_template_date', '2021-09-30')
MGNIFY_DATABASE_PATH = get_af3_flag_value('--mgnify_database_path', os.path.join('/root/public_databases','mgy_clusters_2022_05.fa'))
NHMMER_BINARY_PATH = get_af3_flag_value('--nhmmer_binary_path', '/hmmer/bin/nhmmer')
NHMMER_N_CPU = get_af3_flag_value('--nhmmer_n_cpu', 8)
NTRNA_DATABASE_PATH = get_af3_flag_value('--ntrna_database_path', os.path.join('/root/public_databases','nt_rna_2023_02_23_clust_seq_id_90_cov_80_rep_seq.fasta'))
NUM_DIFFUSION_SAMPLES = get_af3_flag_value('--num_diffusion_samples', 5)
NUM_RECYCLES = get_af3_flag_value('--num_recycles', 10)
if "--num_seeds" in config["af3_flags"]:
    NUM_SEEDS_ARG = f"--num_seeds={config['af3_flags']['--num_seeds']}"
else:
    NUM_SEEDS_ARG = f""

PDB_DATABASE_PATH = get_af3_flag_value('--pdb_database_path', os.path.join('/root/public_databases','mmcif_files'))
RFAM_DATABASE_PATH = get_af3_flag_value('--rfam_database_path', os.path.join('/root/public_databases','rfam_14_9_clust_seq_id_90_cov_80_rep_seq.fasta'))
RNA_CENTRAL_DATABASE_PATH = get_af3_flag_value('--rna_central_database_path', os.path.join('/root/public_databases','rnacentral_active_seq_id_90_cov_80_linclust.fasta'))
SAVE_EMBEDDINGS = get_af3_flag_value('--save_embeddings', False)
SEQRES_DATABASE_PATH = get_af3_flag_value('--seqres_database_path', os.path.join('/root/public_databases','pdb_seqres_2022_09_28.fasta'))
SMALL_BFD_DATABASE_PATH = get_af3_flag_value('--small_bfd_database_path', os.path.join('/root/public_databases','bfd-first_non_consensus_sequences.fasta'))
UNIPROT_CLUSTER_ANNOT_DATABASE_PATH = get_af3_flag_value('--uniprot_cluster_annot_database_path', os.path.join('/root/public_databases','uniprot_all_2021_04.fa'))
UNIREF90_DATABASE_PATH = get_af3_flag_value('--uniref90_database_path', os.path.join('/root/public_databases','uniref90_2022_05.fa'))

def get_preprocessing_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES, = glob_wildcards(os.path.join(PREPROCESSING_DIR, "{i}.json"))
    return list(expand(os.path.join(PREPROCESSING_DIR,"{i}.json"),i=JOB_NAMES))

def get_individual_jobs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES, = glob_wildcards(os.path.join(PREPROCESSING_DIR, "{i}.json"))
    return list(expand(os.path.join(OUTPUT_DIR,"CREATE_AF3_INFERENCE_JOBS","{i}_af3_inference_job.txt"),i=JOB_NAMES))

def get_data_pipeline_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"{i}.json"))
    return list(expand(os.path.join(OUTPUT_DIR,"AF3_DATA_PIPELINE","{i}/{i}_data.json"),i=JOB_NAMES))

def get_multimeric_json_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
#    JOB_NAMES_MONOMERS, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"monomers","{i}.json"))
    JOB_NAMES_MULTIMERS, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"multimers","{multi}.json"))
    print("JOB_NAMES=",JOB_NAMES_MULTIMERS[:10])
    return list(expand(os.path.join(PREPROCESSING_DIR,"multimers","{multi}.json"),multi=JOB_NAMES_MULTIMERS))

def get_monomeric_json_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES_MONOMERS, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"monomers","{mono}.json"))
    print("JOB_NAMES=",JOB_NAMES_MONOMERS[:10])
    return list(expand(os.path.join(OUTPUT_DIR,"AF3_DATA_PIPELINE","monomers","{mono}/{mono}_data.json"),mono=JOB_NAMES_MONOMERS))

def get_multimeric_json_with_msas(wildcards):
    MULTIMERIC_JSON_DIR = checkpoints.MERGE_MONO_AND_MULTI_JSON.get(**wildcards).output[0]
    JOB_NAMES_MULTIMERS_WITH_MSAS, = glob_wildcards(os.path.join(MULTIMERIC_JSON_DIR,"{multi_with_msa}","{multi_with_msa}.json"))
    print("JOB_NAMES=",JOB_NAMES_MULTIMERS_WITH_MSAS[:10])
    return list(expand(os.path.join(OUTPUT_DIR,"MERGE_MONO_AND_MULTI_JSON","{multi_with_msa}/{multi_with_msa}_data.json"),
        multi_with_msa=JOB_NAMES_MULTIMERS_WITH_MSAS))

def get_multimeric_inference_jobs(wildcards):
    MULTIMERIC_JSON_DIR = checkpoints.MERGE_MONO_AND_MULTI_JSON.get(**wildcards).output[0]
    JOB_NAMES_MULTIMERS_WITH_MSAS, = glob_wildcards(os.path.join(MULTIMERIC_JSON_DIR,"{multi}","{multi}_data.json"))
    print("JOB_NAMES=",MULTIMERIC_JSON_DIR)
    print("JOB_NAMES=",JOB_NAMES_MULTIMERS_WITH_MSAS[:10])
    return list(expand(os.path.join(OUTPUT_DIR,"CREATE_AF3_INFERENCE_JOBS","{multi}_af3_inference_job.txt"),  multi=JOB_NAMES_MULTIMERS_WITH_MSAS))


rule all:
    input:
        "output/AGG_AF3_INFERENCE_JOBS/af3_inference_jobs.txt",
        get_individual_jobs if MODE == "default" else[],
        get_multimeric_json_with_msas if MODE in ["all-vs-all", "pulldown"] else [],
        get_preprocessing_outputs,
        expand(os.path.join(OUTPUT_DIR,"AF3_INFERENCE",f"af3_inference_jobs_{{s}}-of-{SPLIT_TOTAL}.done.txt"),s=list(range(1,SPLIT_TOTAL+1))),

checkpoint PREPROCESSING:
    input:
        INPUT_DF
    output:
        directory(os.path.join(OUTPUT_DIR,"PREPROCESSING"))
    params:
        msa_option = MSA_OPTION,
        mode = MODE
    shell:
        """
        if [[ "{params.msa_option}" == "auto" ]]; then
            python {WORKFLOW_DIR}/scripts/create_tasks_from_dataframe.py {input} {output} --msa-option auto_template_based --mode {params.mode}
            python {WORKFLOW_DIR}/scripts/create_tasks_from_dataframe.py {input} {output} --msa-option auto_template_free --mode {params.mode}
        fi
        if [[ "{params.msa_option}" == "auto_template_free" ]]; then
            python {WORKFLOW_DIR}/scripts/create_tasks_from_dataframe.py {input} {output} --msa-option auto_template_free --mode {params.mode}
        fi
        if [[ "{params.msa_option}" == "auto_template_based" ]]; then
            python {WORKFLOW_DIR}/scripts/create_tasks_from_dataframe.py {input} {output} --msa-option auto_template_based --mode {params.mode}
        fi
        if [[ "{params.msa_option}" == "custom" ]]; then
            python {WORKFLOW_DIR}/scripts/create_tasks_from_dataframe.py {input} {output} --mode {params.mode}
        fi
        """

rule AF3_DATA_DEFAULT_PIPELINE:
    input:
        default = os.path.join(OUTPUT_DIR,"PREPROCESSING","{i}.json") if MODE == "default" else [],
    params:
        mode = MODE,
        buckets = BUCKETS,
        conformer_max_iterations = CONFORMER_MAX_ITERATIONS,
        flash_attention_implementation = FLASH_ATTENTION_IMPLEMENTATION,
        gpu_device = GPU_DEVICE,
        hmmalign_binary_path = HMMALIGN_BINARY_PATH,
        hmmbuild_binary_path = HMMBUILD_BINARY_PATH,
        hmmsearch_binary_path = HMMSEARCH_BINARY_PATH,
        jackhmmer_binary_path = JACKHMMER_BINARY_PATH,
        jackhmmer_n_cpu = JACKHMMER_N_CPU,
        jax_compilation_cache_dir = JAX_COMPILATION_CACHE_DIR,
        max_template_date = MAX_TEMPLATE_DATE,
        mgnify_database_path = MGNIFY_DATABASE_PATH,
        nhmmer_binary_path = NHMMER_BINARY_PATH,
        nhmmer_n_cpu = NHMMER_N_CPU,
        ntrna_database_path = NTRNA_DATABASE_PATH,
        num_diffusion_samples = NUM_DIFFUSION_SAMPLES,
        num_recycles = NUM_RECYCLES,
        num_seeds_arg = NUM_SEEDS_ARG,
        pdb_database_path = PDB_DATABASE_PATH,
        rfam_database_path = RFAM_DATABASE_PATH,
        rna_central_database_path = RNA_CENTRAL_DATABASE_PATH,
        save_embeddings = SAVE_EMBEDDINGS,
        seqres_database_path = SEQRES_DATABASE_PATH,
        small_bfd_database_path = SMALL_BFD_DATABASE_PATH,
        uniprot_cluster_annot_database_path = UNIPROT_CLUSTER_ANNOT_DATABASE_PATH,
        uniref90_database_path = UNIREF90_DATABASE_PATH
    output:
        data_pipeline_msa=os.path.join(OUTPUT_DIR,"AF3_DATA_PIPELINE","{i}/{i}_data.json") if MODE == "default" else [],
    container:
        AF3_CONTAINER
    shell:
        """
        python /app/alphafold/run_alphafold.py --json_path=/root/af_output/PREPROCESSING/{wildcards.i}.json \
        --model_dir=/root/models \
        --output_dir=/root/af_output/AF3_DATA_PIPELINE \
        --db_dir=/root/public_databases \
        --run_data_pipeline=true \
        --run_inference=false \
        --buckets={params.buckets} \
        --conformer_max_iterations={params.conformer_max_iterations} \
        --flash_attention_implementation={params.flash_attention_implementation} \
        --gpu_device={params.gpu_device} \
        --hmmalign_binary_path={params.hmmalign_binary_path} \
        --hmmbuild_binary_path={params.hmmbuild_binary_path} \
        --hmmsearch_binary_path={params.hmmsearch_binary_path} \
        --jackhmmer_binary_path={params.jackhmmer_binary_path} \
        --jackhmmer_n_cpu={params.jackhmmer_n_cpu} \
        --jax_compilation_cache_dir={params.jax_compilation_cache_dir} \
        --max_template_date={params.max_template_date} \
        --mgnify_database_path={params.mgnify_database_path} \
        --nhmmer_binary_path={params.nhmmer_binary_path} \
        --nhmmer_n_cpu={params.nhmmer_n_cpu} \
        --ntrna_database_path={params.ntrna_database_path} \
        --num_diffusion_samples={params.num_diffusion_samples} \
        --num_recycles={params.num_recycles} \
        {params.num_seeds_arg} \
        --pdb_database_path={params.pdb_database_path} \
        --rfam_database_path={params.rfam_database_path} \
        --rna_central_database_path={params.rna_central_database_path} \
        --save_embeddings={params.save_embeddings} \
        --seqres_database_path={params.seqres_database_path} \
        --small_bfd_database_path={params.small_bfd_database_path} \
        --uniprot_cluster_annot_database_path={params.uniprot_cluster_annot_database_path} \
        --uniref90_database_path={params.uniref90_database_path}
        """

rule AF3_DATA_SPEEDY_PIPELINE:
    input:
        monomers = os.path.join(OUTPUT_DIR,"PREPROCESSING","monomers","{mono}.json") if MODE in ["all-vs-all","pulldown"]  else [],
    params:
        mode = MODE,
        buckets = BUCKETS,
        conformer_max_iterations = CONFORMER_MAX_ITERATIONS,
        flash_attention_implementation = FLASH_ATTENTION_IMPLEMENTATION,
        gpu_device = GPU_DEVICE,
        hmmalign_binary_path = HMMALIGN_BINARY_PATH,
        hmmbuild_binary_path = HMMBUILD_BINARY_PATH,
        hmmsearch_binary_path = HMMSEARCH_BINARY_PATH,
        jackhmmer_binary_path = JACKHMMER_BINARY_PATH,
        jackhmmer_n_cpu = JACKHMMER_N_CPU,
        jax_compilation_cache_dir = JAX_COMPILATION_CACHE_DIR,
        max_template_date = MAX_TEMPLATE_DATE,
        mgnify_database_path = MGNIFY_DATABASE_PATH,
        nhmmer_binary_path = NHMMER_BINARY_PATH,
        nhmmer_n_cpu = NHMMER_N_CPU,
        ntrna_database_path = NTRNA_DATABASE_PATH,
        num_diffusion_samples = NUM_DIFFUSION_SAMPLES,
        num_recycles = NUM_RECYCLES,
        num_seeds_arg = NUM_SEEDS_ARG,
        pdb_database_path = PDB_DATABASE_PATH,
        rfam_database_path = RFAM_DATABASE_PATH,
        rna_central_database_path = RNA_CENTRAL_DATABASE_PATH,
        save_embeddings = SAVE_EMBEDDINGS,
        seqres_database_path = SEQRES_DATABASE_PATH,
        small_bfd_database_path = SMALL_BFD_DATABASE_PATH,
        uniprot_cluster_annot_database_path = UNIPROT_CLUSTER_ANNOT_DATABASE_PATH,
        uniref90_database_path = UNIREF90_DATABASE_PATH
    output:
        data_pipeline_monomers=os.path.join(OUTPUT_DIR,"AF3_DATA_PIPELINE","monomers","{mono}/{mono}_data.json") if MODE in [
            "all-vs-all", "pulldown"] else [],
    container:
        AF3_CONTAINER
    shell:
        """
        python /app/alphafold/run_alphafold.py --json_path=/root/af_output/PREPROCESSING/monomers/{wildcards.mono}.json \
        --model_dir=/root/models \
        --output_dir=/root/af_output/AF3_DATA_PIPELINE/monomers \
        --db_dir=/root/public_databases \
        --run_data_pipeline=true \
        --run_inference=false \
        --buckets={params.buckets} \
        --conformer_max_iterations={params.conformer_max_iterations} \
        --flash_attention_implementation={params.flash_attention_implementation} \
        --gpu_device={params.gpu_device} \
        --hmmalign_binary_path={params.hmmalign_binary_path} \
        --hmmbuild_binary_path={params.hmmbuild_binary_path} \
        --hmmsearch_binary_path={params.hmmsearch_binary_path} \
        --jackhmmer_binary_path={params.jackhmmer_binary_path} \
        --jackhmmer_n_cpu={params.jackhmmer_n_cpu} \
        --jax_compilation_cache_dir={params.jax_compilation_cache_dir} \
        --max_template_date={params.max_template_date} \
        --mgnify_database_path={params.mgnify_database_path} \
        --nhmmer_binary_path={params.nhmmer_binary_path} \
        --nhmmer_n_cpu={params.nhmmer_n_cpu} \
        --ntrna_database_path={params.ntrna_database_path} \
        --num_diffusion_samples={params.num_diffusion_samples} \
        --num_recycles={params.num_recycles} \
        {params.num_seeds_arg} \
        --pdb_database_path={params.pdb_database_path} \
        --rfam_database_path={params.rfam_database_path} \
        --rna_central_database_path={params.rna_central_database_path} \
        --save_embeddings={params.save_embeddings} \
        --seqres_database_path={params.seqres_database_path} \
        --small_bfd_database_path={params.small_bfd_database_path} \
        --uniprot_cluster_annot_database_path={params.uniprot_cluster_annot_database_path} \
        --uniref90_database_path={params.uniref90_database_path}
        """

#    AF3_CONTAINER
# shell:
    #     if [[ "{params.mode}" == "default" ]]; then
    #         json_path=/root/af_output/PREPROCESSING/{wildcards.i}.json
    #         output_dir=/root/af_output/AF3_DATA_PIPELINE
    #     fi
    #     if [[ "{params.mode}" == "pulldown" || "{params.mode}" == "all-vs-all" ]]; then
    #         json_path=/root/af_output/PREPROCESSING/monomers/{wildcards.mono}.json
    #         output_dir=/root/af_output/AF3_DATA_PIPELINE/monomers
    #     fi
    #     python /app/alphafold/run_alphafold.py --json_path=$json_path \
    #     --model_dir=/root/models \
    #     --output_dir=$output_dir \
    #     --db_dir=/root/public_databases \
    #     --run_data_pipeline=true \
    #     --run_inference=false \
    #     --buckets={params.buckets} \
    #     --conformer_max_iterations={params.conformer_max_iterations} \
    #     --flash_attention_implementation={params.flash_attention_implementation} \
    #     --gpu_device={params.gpu_device} \
    #     --hmmalign_binary_path={params.hmmalign_binary_path} \
    #     --hmmbuild_binary_path={params.hmmbuild_binary_path} \
    #     --hmmsearch_binary_path={params.hmmsearch_binary_path} \
    #     --jackhmmer_binary_path={params.jackhmmer_binary_path} \
    #     --jackhmmer_n_cpu={params.jackhmmer_n_cpu} \
    #     --jax_compilation_cache_dir={params.jax_compilation_cache_dir} \
    #     --max_template_date={params.max_template_date} \
    #     --mgnify_database_path={params.mgnify_database_path} \
    #     --nhmmer_binary_path={params.nhmmer_binary_path} \
    #     --nhmmer_n_cpu={params.nhmmer_n_cpu} \
    #     --ntrna_database_path={params.ntrna_database_path} \
    #     --num_diffusion_samples={params.num_diffusion_samples} \
    #     --num_recycles={params.num_recycles} \
    #     {params.num_seeds_arg} \
    #     --pdb_database_path={params.pdb_database_path} \
    #     --rfam_database_path={params.rfam_database_path} \
    #     --rna_central_database_path={params.rna_central_database_path} \
    #     --save_embeddings={params.save_embeddings} \
    #     --seqres_database_path={params.seqres_database_path} \
    #     --small_bfd_database_path={params.small_bfd_database_path} \
    #     --uniprot_cluster_annot_database_path={params.uniprot_cluster_annot_database_path} \
    #     --uniref90_database_path={params.uniref90_database_path}
    #     """

checkpoint MERGE_MONO_AND_MULTI_JSON:
    input:
        data_pipeline = get_data_pipeline_outputs if MODE == "default" else [],
        data_pipeline_monomers = get_monomeric_json_outputs if MODE in ["all-vs-all","pulldown"] else[],
        preprocessed_multimers = get_multimeric_json_outputs if MODE in ["all-vs-all","pulldown"] else[],
    params:
        mode=MODE,
    output:
        data_pipeline_msa = directory(os.path.join(OUTPUT_DIR,"MERGE_MONO_AND_MULTI_JSON")) if MODE in ["all-vs-all","pulldown"] else [],
    shell:
        """
        python {WORKFLOW_DIR}/scripts/merge_mono_and_multi_jsons.py {input.data_pipeline_monomers} {input.preprocessed_multimers} {output}
        """


rule CREATE_AF3_INFERENCE_JOBS_DEFAULT_AF3_PIPELINE:
    input:
        data_pipeline_msa = os.path.join(OUTPUT_DIR,"AF3_DATA_PIPELINE","{i}/{i}_data.json") if MODE == "default" else [],
    params:
        mode = MODE,
        buckets = BUCKETS,
        conformer_max_iterations = CONFORMER_MAX_ITERATIONS,
        flash_attention_implementation = FLASH_ATTENTION_IMPLEMENTATION,
        gpu_device = GPU_DEVICE,
        hmmalign_binary_path = HMMALIGN_BINARY_PATH,
        hmmbuild_binary_path = HMMBUILD_BINARY_PATH,
        hmmsearch_binary_path = HMMSEARCH_BINARY_PATH,
        jackhmmer_binary_path = JACKHMMER_BINARY_PATH,
        jackhmmer_n_cpu = JACKHMMER_N_CPU,
        jax_compilation_cache_dir = JAX_COMPILATION_CACHE_DIR,
        max_template_date = MAX_TEMPLATE_DATE,
        mgnify_database_path = MGNIFY_DATABASE_PATH,
        nhmmer_binary_path = NHMMER_BINARY_PATH,
        nhmmer_n_cpu = NHMMER_N_CPU,
        ntrna_database_path = NTRNA_DATABASE_PATH,
        num_diffusion_samples = NUM_DIFFUSION_SAMPLES,
        num_recycles = NUM_RECYCLES,
        num_seeds_arg = NUM_SEEDS_ARG,
        pdb_database_path = PDB_DATABASE_PATH,
        rfam_database_path = RFAM_DATABASE_PATH,
        rna_central_database_path = RNA_CENTRAL_DATABASE_PATH,
        save_embeddings = SAVE_EMBEDDINGS,
        seqres_database_path = SEQRES_DATABASE_PATH,
        small_bfd_database_path = SMALL_BFD_DATABASE_PATH,
        uniprot_cluster_annot_database_path = UNIPROT_CLUSTER_ANNOT_DATABASE_PATH,
        uniref90_database_path = UNIREF90_DATABASE_PATH
    output:
        data_pipeline_msa = os.path.join(OUTPUT_DIR,"CREATE_AF3_INFERENCE_JOBS","{i}_af3_inference_job.txt") if MODE == "default" else [],
    shell:
        """
        json_path=/root/af_output/AF3_DATA_PIPELINE/{wildcards.i}/{wildcards.i}_data.json
        echo 'python /app/alphafold/run_alphafold.py --json_path=$json_path \
        --model_dir=/root/models \
        --output_dir=/root/af_output/AF3_INFERENCE \
        --db_dir=/root/public_databases \
        --run_data_pipeline=false \
        --run_inference=true \
        --buckets={params.buckets} \
        --conformer_max_iterations={params.conformer_max_iterations} \
        --flash_attention_implementation={params.flash_attention_implementation} \
        --gpu_device={params.gpu_device} \
        --hmmalign_binary_path={params.hmmalign_binary_path} \
        --hmmbuild_binary_path={params.hmmbuild_binary_path} \
        --hmmsearch_binary_path={params.hmmsearch_binary_path} \
        --jackhmmer_binary_path={params.jackhmmer_binary_path} \
        --jackhmmer_n_cpu={params.jackhmmer_n_cpu} \
        --jax_compilation_cache_dir={params.jax_compilation_cache_dir} \
        --max_template_date={params.max_template_date} \
        --mgnify_database_path={params.mgnify_database_path} \
        --nhmmer_binary_path={params.nhmmer_binary_path} \
        --nhmmer_n_cpu={params.nhmmer_n_cpu} \
        --ntrna_database_path={params.ntrna_database_path} \
        --num_diffusion_samples={params.num_diffusion_samples} \
        --num_recycles={params.num_recycles} \
        {params.num_seeds_arg} \
        --pdb_database_path={params.pdb_database_path} \
        --rfam_database_path={params.rfam_database_path} \
        --rna_central_database_path={params.rna_central_database_path} \
        --save_embeddings={params.save_embeddings} \
        --seqres_database_path={params.seqres_database_path} \
        --small_bfd_database_path={params.small_bfd_database_path} \
        --uniprot_cluster_annot_database_path={params.uniprot_cluster_annot_database_path} \
        --uniref90_database_path={params.uniref90_database_path}' > {output}
        """

rule CREATE_AF3_INFERENCE_JOBS_SPEEDY_AF3_PIPELINE:
    input:
        multimers = os.path.join(OUTPUT_DIR,"MERGE_MONO_AND_MULTI_JSON","{multi}","{multi}_data.json") if MODE in ["all-vs-all","pulldown"]  else []
    params:
        mode = MODE,
        buckets = BUCKETS,
        conformer_max_iterations = CONFORMER_MAX_ITERATIONS,
        flash_attention_implementation = FLASH_ATTENTION_IMPLEMENTATION,
        gpu_device = GPU_DEVICE,
        hmmalign_binary_path = HMMALIGN_BINARY_PATH,
        hmmbuild_binary_path = HMMBUILD_BINARY_PATH,
        hmmsearch_binary_path = HMMSEARCH_BINARY_PATH,
        jackhmmer_binary_path = JACKHMMER_BINARY_PATH,
        jackhmmer_n_cpu = JACKHMMER_N_CPU,
        jax_compilation_cache_dir = JAX_COMPILATION_CACHE_DIR,
        max_template_date = MAX_TEMPLATE_DATE,
        mgnify_database_path = MGNIFY_DATABASE_PATH,
        nhmmer_binary_path = NHMMER_BINARY_PATH,
        nhmmer_n_cpu = NHMMER_N_CPU,
        ntrna_database_path = NTRNA_DATABASE_PATH,
        num_diffusion_samples = NUM_DIFFUSION_SAMPLES,
        num_recycles = NUM_RECYCLES,
        num_seeds_arg = NUM_SEEDS_ARG,
        pdb_database_path = PDB_DATABASE_PATH,
        rfam_database_path = RFAM_DATABASE_PATH,
        rna_central_database_path = RNA_CENTRAL_DATABASE_PATH,
        save_embeddings = SAVE_EMBEDDINGS,
        seqres_database_path = SEQRES_DATABASE_PATH,
        small_bfd_database_path = SMALL_BFD_DATABASE_PATH,
        uniprot_cluster_annot_database_path = UNIPROT_CLUSTER_ANNOT_DATABASE_PATH,
        uniref90_database_path = UNIREF90_DATABASE_PATH
    output:
        multimers = os.path.join(OUTPUT_DIR,"CREATE_AF3_INFERENCE_JOBS","{multi}_af3_inference_job.txt") if MODE in ["all-vs-all","pulldown"]  else []
    shell:
        """
        echo 'python /app/alphafold/run_alphafold.py --json_path=/root/af_output/MERGE_MONO_AND_MULTI_JSON/{wildcards.multi}/{wildcards.multi}_data.json \
        --model_dir=/root/models \
        --output_dir=/root/af_output/AF3_INFERENCE \
        --db_dir=/root/public_databases \
        --run_data_pipeline=false \
        --run_inference=true \
        --buckets={params.buckets} \
        --conformer_max_iterations={params.conformer_max_iterations} \
        --flash_attention_implementation={params.flash_attention_implementation} \
        --gpu_device={params.gpu_device} \
        --hmmalign_binary_path={params.hmmalign_binary_path} \
        --hmmbuild_binary_path={params.hmmbuild_binary_path} \
        --hmmsearch_binary_path={params.hmmsearch_binary_path} \
        --jackhmmer_binary_path={params.jackhmmer_binary_path} \
        --jackhmmer_n_cpu={params.jackhmmer_n_cpu} \
        --jax_compilation_cache_dir={params.jax_compilation_cache_dir} \
        --max_template_date={params.max_template_date} \
        --mgnify_database_path={params.mgnify_database_path} \
        --nhmmer_binary_path={params.nhmmer_binary_path} \
        --nhmmer_n_cpu={params.nhmmer_n_cpu} \
        --ntrna_database_path={params.ntrna_database_path} \
        --num_diffusion_samples={params.num_diffusion_samples} \
        --num_recycles={params.num_recycles} \
        {params.num_seeds_arg} \
        --pdb_database_path={params.pdb_database_path} \
        --rfam_database_path={params.rfam_database_path} \
        --rna_central_database_path={params.rna_central_database_path} \
        --save_embeddings={params.save_embeddings} \
        --seqres_database_path={params.seqres_database_path} \
        --small_bfd_database_path={params.small_bfd_database_path} \
        --uniprot_cluster_annot_database_path={params.uniprot_cluster_annot_database_path} \
        --uniref90_database_path={params.uniref90_database_path}' > {output}
        """


rule AGG_AF3_INFERENCE_JOBS:
    input:
        individual_jobs = get_individual_jobs if MODE == "default" else [],
        multimer_individual_jobs = get_multimeric_inference_jobs if MODE in ["all-vs-all","pulldown"] else [],
    params:
        mode = MODE
    output:
        job_list = os.path.join(OUTPUT_DIR,"AGG_AF3_INFERENCE_JOBS","af3_inference_jobs.txt")
    shell:
        """
        if [[ "{params.mode}" == "default" ]]; then
                cat {input.individual_jobs} > {output.job_list}
        fi
        if [[ "{params.mode}" == "pulldown" || "{params.mode}" == "all-vs-all" ]]; then
                cat {input.multimer_individual_jobs} >> {output.job_list}
        fi
        """

rule SPLIT_INFERENCE_JOB_LIST:
    input:
        inference_job_list = os.path.join(OUTPUT_DIR,"AGG_AF3_INFERENCE_JOBS","af3_inference_jobs.txt")
    params:
        split_total = SPLIT_TOTAL
    output:
        sub_job_list = scatter.split(os.path.join(OUTPUT_DIR,"SPLIT_INFERENCE_JOB_LIST","af3_inference_jobs_{scatteritem}.txt"))
    run:
        import os
        lines = open(input.inference_job_list).readlines()
        [open(output.sub_job_list[i], "w").writelines(lines[i*len(lines)//params.split_total:(i+1)*len(lines)//params.split_total + (1 if i == 3 else 0)])
         for i in range(params.split_total)]



rule AF3_INFERENCE:
    input:
        job_list = os.path.join(OUTPUT_DIR,"SPLIT_INFERENCE_JOB_LIST","af3_inference_jobs_{scatteritem}.txt"),
        data_pipeline_msa= get_data_pipeline_outputs if MODE == "default" else [],
        data_pipeline_msa_multimers = get_multimeric_json_with_msas if MODE in ["all-vs-all","pulldown"] else [],
    output:
        touch(os.path.join(OUTPUT_DIR,"AF3_INFERENCE","af3_inference_jobs_{scatteritem}.done.txt")),
    container:
        AF3_CONTAINER
    shell:
        """
        bash workflow/scripts/parallel.sh /root/af_output/SPLIT_INFERENCE_JOB_LIST/af3_inference_jobs_{wildcards.scatteritem}.txt
        """
