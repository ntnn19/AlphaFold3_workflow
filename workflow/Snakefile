#print("Step #1: AlphaFold3 preprocessing + data pipeline")
import string
from pathlib import Path
import pandas as pd
import os
import yaml

localrules: PREPROCESSING , MERGE_MONO_AND_MULTI_JSON, CREATE_AF3_INFERENCE_JOBS_SPEEDY_AF3_PIPELINE, AGG_AF3_INFERENCE_JOBS, SPLIT_INFERENCE_JOB_LIST


# Define expected schema for each sample sheet type
SAMPLE_SHEET_SCHEMAS = {
    "raw_data": [
        "job_name", "type", "id", "sequence", "modifications", "ccd_codes",
        "smiles", "msa_option", "unpaired_msa", "paired_msa", "templates",
        "model_seeds", "bonded_atom_pairs", "user_ccd"
    ],
    "data_pipeline_ready": ["sample_id", "file"],
    "inference_ready": ["sample_id", "file"],
    "merge_ready_samples": ["sample_id", "multimer_file", "monomer_chain_id", "monomer_file"],
}


scattergather:
    split=config.get("n_splits",1),
    split_ost=config.get("n_splits_ost",1)

def as_bool(val):
    if isinstance(val, bool):
        return val
    if isinstance(val, (int, float)):
        return bool(val)
    if isinstance(val, str):
        return val.strip().lower() in {"1", "true", "yes", "y", "on"}
    return False

def bool_flag(name, value):
    return f"--{name}" if as_bool(value) else ""


def load_sample_sheet(sheet_key):
    """Safely load a sample sheet, returns (path, dataframe)"""
    path = config.get("sample_sheets",{}).get(sheet_key)

    # Get expected columns for this sheet type
    columns = SAMPLE_SHEET_SCHEMAS.get(sheet_key,[])

    # Return None, empty DataFrame with correct columns if not specified
    if not path:
        return None, pd.DataFrame(columns=columns)

    # Check if file exists
    if not Path(path).exists():
        print(f"Warning: Sample sheet '{sheet_key}' not found at {path}")
        return path, pd.DataFrame(columns=columns)

    # Try to read
    try:
        df = pd.read_csv(path,sep="\t")
        print(f"✓ Loaded {len(df)} samples from '{sheet_key}' ({path})")
        return path, df
    except Exception as e:
        print(f"✗ Error reading '{sheet_key}' from {path}: {e}")
        return path, pd.DataFrame(columns=columns)




def as_bool(val):
    if isinstance(val, bool):
        return val
    if isinstance(val, (int, float)):
        return bool(val)
    if isinstance(val, str):
        return val.strip().lower() in {"1", "true", "yes", "y", "on"}
    return False

def bool_flag(name, value):
    return f"--{name}" if as_bool(value) else ""

def sanitised_name(name):
    """Returns sanitised version of the name that can be used as a filename."""
    lower_spaceless_name = name.lower().replace(' ', '_')
    allowed_chars = set(string.ascii_lowercase + string.digits + '_-.')
    return ''.join(l for l in lower_spaceless_name if l in allowed_chars)


def get_preprocessing_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES, = glob_wildcards(os.path.join(PREPROCESSING_DIR, "{i}.json"))
    return list(expand(os.path.join(PREPROCESSING_DIR,"{i}.json"),i=JOB_NAMES))

def get_individual_jobs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES, = glob_wildcards(os.path.join(PREPROCESSING_DIR, "{i}.json"))
    return list(expand(os.path.join(OUTPUT_DIR,"CREATE_AF3_INFERENCE_JOBS","{i}_af3_inference_job.txt"),i=JOB_NAMES))

def get_data_pipeline_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"{i}.json"))
    return list(expand(os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","{i}/{i}_data.json"),i=JOB_NAMES))

def get_multimeric_json_outputs(wildcards):
    print("HERE")
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES_MULTIMERS, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"multimers","{multi}.json"))
    return list(expand(os.path.join(PREPROCESSING_DIR,"multimers","{multi}.json"),multi=JOB_NAMES_MULTIMERS)) + list(expand(os.path.join(OUTPUT_DIR,"rule_MERGE_MONOMERS_TO_MULTIMERS","{multi}_data.json"), multi=JOB_NAMES_MULTIMERS))

def get_monomeric_json_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES_MONOMERS, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"monomers","{mono}.json"))
    print(JOB_NAMES_MONOMERS)
#    return list(expand(os.path.join(OUTPUT_DIR,"AF3_DATA_PIPELINE_TEST","monomers","{mono}/{mono}_data.json"),mono=JOB_NAMES_MONOMERS))
    return list(expand(os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","{mono}/{mono}_data.json"),mono=JOB_NAMES_MONOMERS))

def get_multi_to_monomeric_dict(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    map_df = pd.read_csv(os.path.join(PREPROCESSING_DIR,"metadata","inference_to_data_pipeline_map.tsv"),sep="\t")
    return map_df

def get_multimeric_json_with_msas(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES_MULTIMERS, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"multimers","{multi}.json"))
    #return list(expand(os.path.join(OUTPUT_DIR,"rule_CREATE_AF3_INFERENCE_JOBS","{multi}_seed-{seed}_af3_inference_job.txt"), multi=JOB_NAMES_MULTIMERS,seed=SEEDS))
    return list(expand(os.path.join(OUTPUT_DIR,"rule_CREATE_AF3_INFERENCE_JOBS","{multi}_af3_inference_job.txt"), multi=JOB_NAMES_MULTIMERS))

def get_collect_predictions(wildcards):
    GET_DONE_OUTPUTS_DIR = checkpoints.GET_DONE_OUTPUTS.get(**wildcards).output[0]
    JOB_NAMES_MULTIMERS, SEEDS_ = glob_wildcards(os.path.join(GET_DONE_OUTPUTS_DIR,"{multi}_seed-{seed}.done.txt"))
    files = list(expand(os.path.join(OUTPUT_DIR,"rule_CREATE_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","{multi}_seed-{seed}_sample-{sample}_job.txt"), multi=set(JOB_NAMES_MULTIMERS),seed=list(set(SEEDS_)), sample=[0,1,2,3,4]))
    return files

def get_af3_flag_value(flag, default_value):
    return config.get('alphafold3_flags', {}).get(flag, default_value)


def get_merge_inputs(wildcards):
    """
    Get inputs for merging. Check user-provided MERGE_READY_DF first,
    otherwise use checkpoint-generated mapping.

    Returns dict with:
    - multimer_template: the multimer JSON file
    - monomer_files: list of processed monomer files (one per chain)
    """

    # Check if user provided this multimer in MERGE_READY_DF
    if not MERGE_READY_DF.empty and wildcards.multi in MERGE_READY_DF['sample_id'].values:
        # Get all rows for this multimer
        multimer_rows = MERGE_READY_DF[MERGE_READY_DF['sample_id'] == wildcards.multi]

        # Get multimer template (same for all rows)
        multimer_template = multimer_rows.iloc[0]['multimer_file']

        # Get all monomer files (one per chain)
        monomer_files = multimer_rows['monomer_file'].tolist()

        return {
            'multimer_template': multimer_template,
            'monomer_files': monomer_files
        }

    # Otherwise, use checkpoint-generated mapping
    checkpoint_output = os.path.join(checkpoints.PREPROCESSING.get(**wildcards).output[0],"metadata","inference_to_data_pipeline_map.tsv")


    #    return list(expand(os.path.join(OUTPUT_DIR,"AF3_DATA_PIPELINE_TEST","monomers","{mono}/{mono}_data.json"),mono=JOB_NAMES_MONOMERS))
    #return list(expand(os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","{mono}/{mono}_data.json"),mono=JOB_NAMES_MONOMERS))
    print("WD=",wildcards)

    mapping = pd.read_csv(checkpoint_output,sep="\t")

    # Get monomers for this multimer
    multimer_rows = mapping[mapping['multimer_file'] == wildcards.multi]
    monomers = multimer_rows['monomer_file'].tolist()

    # Multimer template from preprocessing
    multimer_template = os.path.join(
        OUTPUT_DIR,
        "rule_PREPROCESSING",
        "multimers",
        f"{wildcards.multi}.json"
    )
    print(wildcards)

    # Get monomer files - check DATA_PIPELINE_READY_DF for each
    monomer_files = get_monomeric_json_outputs(wildcards)
    for mono in monomers:
             # Use pipeline output
        mono_file = os.path.join(
            OUTPUT_DIR,
            "rule_AF3_DATA_PIPELINE",
            mono,
            f"{mono}_data.json"
        )
        monomer_files.append(mono_file)
    print(multimer_template,monomer_files)
    #exit()
    return {
        'multimer_template': multimer_template,
        'monomer_files': monomer_files
    }

SPLIT_TOTAL = workflow._scatter["split"]
SPLIT_OST = workflow._scatter["split_ost"]



# Load all sample sheets - get both paths and DataFrames
# TODO fill the empty dataframes for all downstream rules if something other than raw_data/inference ready was provided
RAW_DATA_PATH, RAW_DATA_DF = load_sample_sheet("raw_data")
DATA_PIPELINE_READY_PATH, DATA_PIPELINE_READY_DF = load_sample_sheet("data_pipeline_ready")
INFERENCE_READY_PATH, INFERENCE_READY_DF = load_sample_sheet("inference_ready")
MERGE_READY_PATH, MERGE_READY_DF = load_sample_sheet("merge_ready")

OUTPUT_DIR = config["output_dir"]
MODE = config.get("mode","custom")
OST_CONTAINER = config.get("ost_container")
GROUND_TRUTH =config.get("ground_truth")
TASK = config.get("task", "")
PREDICT_INDIVIDUAL_COMPONENTS = bool_flag("predict_individual_components",config.get("predict_individual_components",False))
N_SEEDS = config.get("n_seeds") # See TODO
N_SAMPLES = config.get("n_samples")




WORKFLOW_DIR = os.path.dirname(os.path.abspath(workflow.snakefile))



if "model_seeds" in RAW_DATA_DF.columns:
    RAW_DATA_DF["model_seeds"] = RAW_DATA_DF["model_seeds"] = RAW_DATA_DF.model_seeds.astype(str)
if RAW_DATA_DF.empty:
    SEEDS =  [1]
else:
    SEEDS= RAW_DATA_DF["model_seeds"].str.split(",").values[0] if "model_seeds" in RAW_DATA_DF.columns else [1] # TODO: ONLY WORKS IF EVERY JOB HAS THE SAME SEEDS
MSA_OPTION = config.get("msa_option","auto")
AF3_CONTAINER = config["af3_flags"]["--af3_container"]



EXTRA_AF3_FLAGS = config.get('alphafold3_flags', {}).get("--extra_af3_flags", '')



rule all:
    input:
        expand(os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","{mono}","{mono}_data.json"),mono=DATA_PIPELINE_READY_DF["sample_id"]),
        os.path.join(OUTPUT_DIR,"rule_AGG_OST_REPORTS","ost_report.csv") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
        expand(os.path.join(OUTPUT_DIR,"rule_OST_COMAPRE_LIGAND_STRUCTURES","done_flags",f"job_{{s}}-of-{SPLIT_OST}.done.txt"),s=list(range(1,SPLIT_OST+1))) if MODE == "virtual-drug-screen" and TASK=="ost" else [],
        expand(os.path.join(OUTPUT_DIR,"rule_AF3_INFERENCE","done_flags",f"af3_inference_jobs_{{s}}-of-{SPLIT_TOTAL}.done.txt"),s=list(range(1,SPLIT_TOTAL+1))),
#        get_monomeric_json_outputs if not RAW_DATA_DF.empty else [],
#        get_multimeric_json_outputs if not RAW_DATA_DF.empty else [],


checkpoint PREPROCESSING:
    input:
        RAW_DATA_PATH if not RAW_DATA_DF.empty else [],
    output:
        directory(os.path.join(OUTPUT_DIR,"rule_PREPROCESSING")) if not RAW_DATA_DF.empty else []
    params:
        msa_option = MSA_OPTION,
        mode = MODE,
        n_seeds = f"--n-seeds {N_SEEDS}" if N_SEEDS else "",
        n_samples = f"--n-samples {N_SAMPLES}" if N_SAMPLES else "",
        predict_individual_components = PREDICT_INDIVIDUAL_COMPONENTS
    shell:
        """
        python {WORKFLOW_DIR}/scripts/preprocessing.py \
        {input[0]} \
        {OUTPUT_DIR} \
        --mode={params.mode} \
        {params.predict_individual_components} {params.n_seeds} {params.n_samples} 
        """



#rule AF3_DATA_SPEEDY_PIPELINE_TEST:
#    input:
#        monomers = os.path.join(OUTPUT_DIR,"rule_PREPROCESSING","monomers","{mono}.json") if MODE in ["custom", "all-vs-all","pulldown","virtual-drug-screen"]  else [],
#    output:
#        data_pipeline_monomers=touch(os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","monomers","{mono}/{mono}_data.json")) if MODE in [
#            "custom","all-vs-all", "pulldown","virtual-drug-screen"] else [],

rule AF3_DATA_SPEEDY_PIPELINE:
    input:
        data = branch(
            lookup(query="sample_id == '{mono}'",within=DATA_PIPELINE_READY_DF ,cols="file"),
            then=lookup(query="sample_id == '{mono}'",within=DATA_PIPELINE_READY_DF ,cols="file"),
            otherwise=os.path.join(OUTPUT_DIR,"rule_PREPROCESSING","monomers","{mono}.json")
        ),
        #monomers = os.path.join(OUTPUT_DIR,"rule_PREPROCESSING","monomers","{mono}.json") if MODE in ["custom",
        #                                                                                              "stoichio-screen",
        #                                                                                              "all-vs-all",
        #                                                                                              "pulldown",
        #                                                                                              "virtual-drug-screen"]  else [],
    params:
        json_path = input.data,
        mode = MODE,
        extra_af3_flags = EXTRA_AF3_FLAGS
    output:
        data_pipeline_monomers=temp(os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","{mono}","{mono}_data.json")) if MODE in [
            "custom","stoichio-screen","all-vs-all", "pulldown","virtual-drug-screen"] else [],
    container:
        AF3_CONTAINER
    shell:
        """
        python /app/alphafold/run_alphafold.py --json_path={params.json_path} \
        --model_dir=/root/models \
        --output_dir=/root/af_output/rule_AF3_DATA_PIPELINE \
        --db_dir=/root/public_databases \
        --run_data_pipeline=true \
        --run_inference=false \
        {params.extra_af3_flags} 
        """

rule MERGE_MONO_AND_MULTI_JSON: # TODO: PROBLEM IS THAT FOR EACH MULTIMER CREATED IN THE PREPROCESS RULE I NEED MULTIPLE MONOMERS CREATED IN THE AF3_DATA_PIPELINE RULE
    input:
        get_monomeric_json_outputs,
        unpack(get_merge_inputs)
        #data=branch(
        #    lookup(query="sample_id == '{mono}'",within=MERGE_READY_DF,cols="file"),
        #    then=lookup(query="sample_id == '{mono}'",within=MERGE_READY_DF,cols="file"),
        #    otherwise=os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","{mono}","{mono}_data.json")
        #)

#        data_pipeline_monomers = lambda wildcards: [os.path.join(OUTPUT_DIR, "rule_AF3_DATA_PIPELINE",f"{os.path.basename(mono).split("_data.json")[0]}",f"{os.path.basename(mono).split("_data.json")[0]}_data.json")  for mono in
#                                                    get_multi_to_monomeric_dict(wildcards).loc[get_multi_to_monomeric_dict(wildcards).inference_samples.str.contains(wildcards.multi),"data_pipeline_samples"].values.tolist()]
#        if MODE in ["all-vs-all", "pulldown","virtual-drug-screen", "stoichio-screen"] else [],
#        preprocessed_multimers = os.path.join(OUTPUT_DIR,"rule_PREPROCESSING","multimers","{multi}.json") if MODE in ["custom","all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"]  else [],
    params:
        metdata_dir = os.path.join(OUTPUT_DIR,"rule_PREPROCESSING","metadata"),
    output:
        #data_pipeline_msa = expand(os.path.join(OUTPUT_DIR,"rule_MERGE_MONOMERS_TO_MULTIMERS","{{multi}}_seed-{seed}_data.json"),seed=SEEDS) if MODE in ["custom","all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"] else [],
        os.path.join(OUTPUT_DIR,"rule_MERGE_MONOMERS_TO_MULTIMERS","{multi}_data.json") if MODE in ["custom","all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"] else [],
    shell:
        """
        python {WORKFLOW_DIR}/scripts/merge_mono_and_multi_jsons.py \
        {params.metdata_dir}/inference_to_data_pipeline_map.tsv \
        {params.metdata_dir}/inference_samples.tsv {wildcards.multi}
        """


rule CREATE_AF3_INFERENCE_JOBS_SPEEDY_AF3_PIPELINE:
    input:
        multimers = os.path.join(OUTPUT_DIR,"rule_MERGE_MONOMERS_TO_MULTIMERS","{multi}_data.json") if MODE in ["custom","all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"]  else []
    params:
        mode = MODE,
        extra_af3_flags = EXTRA_AF3_FLAGS
    output:
        multimers = os.path.join(OUTPUT_DIR,"rule_CREATE_AF3_INFERENCE_JOBS","{multi}_af3_inference_job.txt") if MODE in ["custom","all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"]  else []
    shell:
        """
        echo 'python /app/alphafold/run_alphafold.py --json_path=/root/af_output/rule_MERGE_MONOMERS_TO_MULTIMERS/{wildcards.multi}_data.json \
        --model_dir=/root/models \
        --output_dir=/root/af_output/rule_AF3_INFERENCE \
        --db_dir=/root/public_databases \
        --run_data_pipeline=false \
        --run_inference=true \
        {params.extra_af3_flags}' > {output}
        """


rule AGG_AF3_INFERENCE_JOBS:
    input:
        merged_multimer = get_multimeric_json_with_msas if MODE in ["custom", "all-vs-all","pulldown","virtual-drug-screen","stoichio-screen","stoichio-screen"]  else [],
    params:
        mode = MODE
    output:
        job_list = os.path.join(OUTPUT_DIR,"rule_AGG_AF3_INFERENCE_JOBS","af3_inference_jobs.txt")
    shell:
        """
        find {OUTPUT_DIR}/rule_CREATE_AF3_INFERENCE_JOBS -type f -name '*.txt' -print0 | tar --null -T - -cf - | tar -xOf - > {output.job_list}
        """

rule SPLIT_INFERENCE_JOB_LIST:
    input:
        inference_job_list = ancient(os.path.join(OUTPUT_DIR,"rule_AGG_AF3_INFERENCE_JOBS","af3_inference_jobs.txt"))
    params:
        split_total = SPLIT_TOTAL
    output:
        sub_job_list = scatter.split(os.path.join(OUTPUT_DIR,"rule_SPLIT_INFERENCE_JOB_LIST","af3_inference_jobs_{scatteritem}.txt"))
    run:
        import os
        lines = open(input.inference_job_list).readlines()
        n_lines = len(lines)
        n_splits = params.split_total

        # Calculate chunk size
        chunk_size = n_lines // n_splits
        remainder = n_lines % n_splits

        start = 0
        for i in range(n_splits):
            # Add one extra line to the first 'remainder' splits
            end = start + chunk_size + (1 if i < remainder else 0)

            with open(output.sub_job_list[i], "w") as f:
                f.writelines(lines[start:end])

            start = end


rule AF3_INFERENCE:
    input:
        job_list = ancient(os.path.join(OUTPUT_DIR,"rule_SPLIT_INFERENCE_JOB_LIST","af3_inference_jobs_{scatteritem}.txt")),
    output:
        touch(os.path.join(OUTPUT_DIR,"rule_AF3_INFERENCE","done_flags","af3_inference_jobs_{scatteritem}.done.txt")),
    container:
        AF3_CONTAINER
    shell:
        """
        bash /scripts/parallel.sh /root/af_output/rule_SPLIT_INFERENCE_JOB_LIST/af3_inference_jobs_{wildcards.scatteritem}.txt
        """


checkpoint GET_DONE_OUTPUTS:
    input:
        expand(os.path.join(OUTPUT_DIR,"rule_AF3_INFERENCE","done_flags",f"af3_inference_jobs_{{s}}-of-{SPLIT_TOTAL}.done.txt"),s=list(range(1,SPLIT_TOTAL+1))),
    output:
        directory(os.path.join(OUTPUT_DIR,"rule_GET_DONE_OUTPUTS"))
    shell:
        """
        mkdir -p {output}
        for i in {{1..{SPLIT_TOTAL}}}; do
            eval \"$(grep -- '--json_path=' {OUTPUT_DIR}/rule_SPLIT_INFERENCE_JOB_LIST/af3_inference_jobs_${{i}}-of-{SPLIT_TOTAL}.txt  | sed -E 's/.*--json_path=\\/.*\\/([^/]+)_data\\.json.*/touch {OUTPUT_DIR}\\/rule_GET_DONE_OUTPUTS\\/\\1.done.txt/')\";
        done
        """

rule COLLECT_AF3_PREDICTIONS:
    input:
        done_files = os.path.join(OUTPUT_DIR,"rule_GET_DONE_OUTPUTS","{multi}_seed-{seed}.done.txt"),
        job = os.path.join(OUTPUT_DIR,"rule_CREATE_AF3_INFERENCE_JOBS","{multi}_seed-{seed}_af3_inference_job.txt") if MODE in ["all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"]  else []
    output:
        os.path.join(OUTPUT_DIR,"COLLECT_AF3_PREDICTIONS","{multi}_seed-{seed}_sample-{sample}_model.cif")
    shell:
        """
        python {WORKFLOW_DIR}/scripts/collect_predictions.py --source-dir {OUTPUT_DIR}/rule_AF3_INFERENCE/{wildcards.multi}_seed-{wildcards.seed} --job-list {input.job} --output-dir {OUTPUT_DIR}/rule_COLLECT_AF3_PREDICTIONS
        """

rule CREATE_OST_COMAPRE_LIGAND_STRUCTURES_JOBS:
    input:
        prediction = os.path.join(OUTPUT_DIR,"COLLECT_AF3_PREDICTIONS","{multi}_seed-{seed}_sample-{sample}_model.cif") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
        ground_truth_cif=lambda wildcards: get_ground_truth_files(wildcards)["cif"] if MODE == "virtual-drug-screen" and TASK=="ost" else [],
        ground_truth_sdf=lambda wildcards: get_ground_truth_files(wildcards)["sdf"] if MODE == "virtual-drug-screen" and TASK=="ost" else []
    output:
        os.path.join(OUTPUT_DIR,"rule_CREATE_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","{multi}_seed-{seed}_sample-{sample}_job.txt") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
    shell:
        """    
        json_path={OUTPUT_DIR}/rule_OST_COMAPRE_LIGAND_STRUCTURES/{wildcards.multi}_seed-{wildcards.seed}_sample-{wildcards.sample}_score.json
        echo "ost compare-ligand-structures \
        -m {input.prediction} \
        -rl {input.ground_truth_sdf} \
        -r {input.ground_truth_cif} \
        -o $json_path \
        --lddt-pli --rmsd --lddt-pli-add-mdl-contacts -ft" > {output}
        """

rule AGG_OST_COMAPRE_LIGAND_STRUCTURES_JOBS:
    input:
        get_collect_predictions,
    output:
        os.path.join(OUTPUT_DIR,"rule_AGG_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","jobs.txt")
    shell:
        """
        cat {input} > {output}
        """

rule SPLIT_OST_COMAPRE_LIGAND_STRUCTURES_JOBS:
    input:
         rules.AGG_OST_COMAPRE_LIGAND_STRUCTURES_JOBS.output if MODE == "virtual-drug-screen" and TASK=="ost" else [],
    params:
         split_ost = SPLIT_OST
    output:
         sub_job_list = scatter.split_ost(os.path.join(OUTPUT_DIR,"rule_SPLIT_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","job_{scatteritem}.txt"))
    run:
        import os
        lines = open(input[0]).readlines()
        [open(output.sub_job_list[i], "w").writelines(lines[i*len(lines)//params.split_ost:(i+1)*len(lines)//params.split_ost + (1 if i == 3 else 0)])
         for i in range(params.split_ost)]


rule OST_COMAPRE_LIGAND_STRUCTURES:
    input:
         sub_job_list = os.path.join(OUTPUT_DIR,"rule_SPLIT_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","job_{scatteritem}.txt") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
    output:
         touch(os.path.join(OUTPUT_DIR,"rule_OST_COMAPRE_LIGAND_STRUCTURES","done_flags","job_{scatteritem}.done.txt")),
    shell:
         """
         bash {WORKFLOW_DIR}/scripts/parallel_cpu.sh {input}
         """

rule CREATE_OST_REPORT:
    input:
         sub_job_list = os.path.join(OUTPUT_DIR,"rule_SPLIT_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","job_{scatteritem}.txt") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
         done_file = os.path.join(OUTPUT_DIR,"rule_OST_COMAPRE_LIGAND_STRUCTURES","done_flags","job_{scatteritem}.done.txt")
    output:
         os.path.join(OUTPUT_DIR,"CREATE_OST_REPORT","{scatteritem}_ost_report.csv")
    shell:
         """
         > {wildcards.scatteritem}.commands.sh
         grep -oP '(?<=-o )\\S+' {input.sub_job_list} | while read -r path; do echo "python {WORKFLOW_DIR}/scripts/af3_analysis.py.bak $path {RUNS_N_POSES_JSON_INPUTS} {OUTPUT_DIR}/AF3_INFERENCE {OUTPUT_DIR}/OST_COMAPRE_LIGAND_STRUCTURES {RUNS_N_POSES_ANNOTATIONS} >> {output}" >> {wildcards.scatteritem}.commands.sh; done
         chmod +x {wildcards.scatteritem}.commands.sh
         bash {WORKFLOW_DIR}/scripts/parallel_cpu.sh {wildcards.scatteritem}.commands.sh
         rm {wildcards.scatteritem}.commands.sh
         """

rule AGG_OST_REPORTS:
    input:
         expand(os.path.join(OUTPUT_DIR,"CREATE_OST_REPORT",f"{{s}}-of-{SPLIT_OST}_ost_report.csv"),s=list(range(1,SPLIT_OST+1))) if MODE == "virtual-drug-screen" and TASK=="ost" else [],
    output:
         os.path.join(OUTPUT_DIR,"rule_AGG_OST_REPORTS","ost_report.csv")
    shell:
        """
        > {output}
        echo "target method seed sample ranking_score prot_lig_chain_iptm_average_lddt_pli prot_lig_chain_iptm_min_lddt_pli prot_lig_chain_iptm_max_lddt_pli lig_prot_chain_iptm_average_lddt_pli lig_prot_chain_iptm_min_lddt_pli lig_prot_chain_iptm_max_lddt_pli lddt_pli model_ligand_chain_lddt_pli model_ligand_ccd_code model_ligand_smiles ligand_ccd_code_x prot_lig_chain_iptm_average_rmsd prot_lig_chain_iptm_min_rmsd prot_lig_chain_iptm_max_rmsd lig_prot_chain_iptm_average_rmsd lig_prot_chain_iptm_min_rmsd lig_prot_chain_iptm_max_rmsd rmsd lddt_lp bb_rmsd model_ligand_chain_rmsd ligand_ccd_code_y ligand_instance_chain ligand_is_proper" > {output}
        cat {input} >> {output}
        """

