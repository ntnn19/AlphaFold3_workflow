#print("Step #1: AlphaFold3 preprocessing + data pipeline")
import string
import pandas as pd
import os
import yaml
localrules: PREPROCESSING , MERGE_MONO_AND_MULTI_JSON, CREATE_AF3_INFERENCE_JOBS_SPEEDY_AF3_PIPELINE, AGG_AF3_INFERENCE_JOBS, SPLIT_INFERENCE_JOB_LIST
scattergather:
    split=config.get("n_splits",1),
    split_ost=config.get("n_splits_ost",1)

def as_bool(val):
    if isinstance(val, bool):
        return val
    if isinstance(val, (int, float)):
        return bool(val)
    if isinstance(val, str):
        return val.strip().lower() in {"1", "true", "yes", "y", "on"}
    return False

def bool_flag(name, value):
    return f"--{name}" if as_bool(value) else ""

SPLIT_TOTAL = workflow._scatter["split"]
SPLIT_OST = workflow._scatter["split_ost"]

#Workflow flags
INPUT_DF = config["input_csv"]
OUTPUT_DIR = config["output_dir"]
MODE = config.get("mode","custom")
OST_CONTAINER = config.get("ost_container",None)
GROUND_TRUTH =config.get("ground_truth",None)
TASK = config.get("task", "")
PREDICT_INDIVIDUAL_COMPONENTS = bool_flag("predict_individual_components",config.get("predict_individual_components",False))
N_SEEDS = config.get("n_seeds", 1)
N_SAMPLES = config.get("n_samples", 5)

# Helper function to get JSON path for each multimer
def claude(wildcards):
    """Get JSON path from sample sheet or from upstream rule"""
    row = INPUT_DF[INPUT_DF['id'] == wildcards.multi]

    if not row.empty and pd.notna(row.iloc[0]['json_path']):
        # Use external JSON if provided
        return row.iloc[0]['json_path']
    else:
        # Fall back to upstream rule output
        return os.path.join(OUTPUT_DIR,"rule_MERGE_MONOMERS_TO_MULTIMERS",
            f"{wildcards.multi}_data.json")

def as_bool(val):
    if isinstance(val, bool):
        return val
    if isinstance(val, (int, float)):
        return bool(val)
    if isinstance(val, str):
        return val.strip().lower() in {"1", "true", "yes", "y", "on"}
    return False

def bool_flag(name, value):
    return f"--{name}" if as_bool(value) else ""


WORKFLOW_DIR = workflow.basedir
#print("WORKFLOW_DIR=", WORKFLOW_DIR)
WORKFLOW_DIR = workflow.source_path(".")

#print("WORKFLOW_DIR=", WORKFLOW_DIR)
WORKFLOW_DIR = workflow.workdir_init

#print("WORKFLOW_DIR=", WORKFLOW_DIR)
WORKFLOW_DIR = os.path.dirname(os.path.abspath(workflow.snakefile))

#print("WORKFLOW_DIR=", WORKFLOW_DIR)
#exit()

DF = pd.read_csv(INPUT_DF)
#SEEDS = (DF["model_seeds"].astype(str).str.split(",").values[0] 
#         if "model_seeds" in DF.columns 
#         else ["1"])
if "model_seeds" in DF.columns:
    DF["model_seeds"] = DF["model_seeds"] = DF.model_seeds.astype(str)
SEEDS= DF["model_seeds"].str.split(",").values[0] if "model_seeds" in DF.columns else [1]
MSA_OPTION = config.get("msa_option","auto")
AF3_CONTAINER = config["af3_flags"]["--af3_container"]

def get_af3_flag_value(flag, default_value):
    return config.get('alphafold3_flags', {}).get(flag, default_value)


EXTRA_AF3_FLAGS = config.get('alphafold3_flags', {}).get("--extra_af3_flags", '')


def sanitised_name(name):
    """Returns sanitised version of the name that can be used as a filename."""
    lower_spaceless_name = name.lower().replace(' ', '_')
    allowed_chars = set(string.ascii_lowercase + string.digits + '_-.')
    return ''.join(l for l in lower_spaceless_name if l in allowed_chars)


def get_preprocessing_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES, = glob_wildcards(os.path.join(PREPROCESSING_DIR, "{i}.json"))
    return list(expand(os.path.join(PREPROCESSING_DIR,"{i}.json"),i=JOB_NAMES))

def get_individual_jobs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES, = glob_wildcards(os.path.join(PREPROCESSING_DIR, "{i}.json"))
    return list(expand(os.path.join(OUTPUT_DIR,"CREATE_AF3_INFERENCE_JOBS","{i}_af3_inference_job.txt"),i=JOB_NAMES))

def get_data_pipeline_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"{i}.json"))
    return list(expand(os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","{i}/{i}_data.json"),i=JOB_NAMES))

def get_multimeric_json_outputs(wildcards):
    print("HERE")
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES_MULTIMERS, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"multimers","{multi}.json"))
    return list(expand(os.path.join(PREPROCESSING_DIR,"multimers","{multi}.json"),multi=JOB_NAMES_MULTIMERS)) + list(expand(os.path.join(OUTPUT_DIR,"rule_MERGE_MONOMERS_TO_MULTIMERS","{multi}_data.json"), multi=JOB_NAMES_MULTIMERS))

def get_monomeric_json_outputs(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES_MONOMERS, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"monomers","{mono}.json"))
    print(JOB_NAMES_MONOMERS)
#    return list(expand(os.path.join(OUTPUT_DIR,"AF3_DATA_PIPELINE_TEST","monomers","{mono}/{mono}_data.json"),mono=JOB_NAMES_MONOMERS))
    return list(expand(os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","{mono}/{mono}_data.json"),mono=JOB_NAMES_MONOMERS))

def get_multi_to_monomeric_dict(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    map_df = pd.read_csv(os.path.join(PREPROCESSING_DIR,"metadata","inference_to_data_pipeline_map.tsv"),sep="\t")
    return map_df

def get_multimeric_json_with_msas(wildcards):
    PREPROCESSING_DIR = checkpoints.PREPROCESSING.get(**wildcards).output[0]
    JOB_NAMES_MULTIMERS, = glob_wildcards(os.path.join(PREPROCESSING_DIR,"multimers","{multi}.json"))
    return list(expand(os.path.join(OUTPUT_DIR,"rule_CREATE_AF3_INFERENCE_JOBS","{multi}_seed-{seed}_af3_inference_job.txt"), multi=JOB_NAMES_MULTIMERS,seed=SEEDS))

def get_collect_predictions(wildcards):
    GET_DONE_OUTPUTS_DIR = checkpoints.GET_DONE_OUTPUTS.get(**wildcards).output[0]
    JOB_NAMES_MULTIMERS, SEEDS_ = glob_wildcards(os.path.join(GET_DONE_OUTPUTS_DIR,"{multi}_seed-{seed}.done.txt"))
    files = list(expand(os.path.join(OUTPUT_DIR,"rule_CREATE_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","{multi}_seed-{seed}_sample-{sample}_job.txt"), multi=set(JOB_NAMES_MULTIMERS),seed=list(set(SEEDS_)), sample=[0,1,2,3,4]))
    return files


rule all:
    input:
        get_monomeric_json_outputs,
        os.path.join(OUTPUT_DIR,"rule_AGG_OST_REPORTS","ost_report.csv") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
        expand(os.path.join(OUTPUT_DIR,"rule_OST_COMAPRE_LIGAND_STRUCTURES","done_flags",f"job_{{s}}-of-{SPLIT_OST}.done.txt"),s=list(range(1,SPLIT_OST+1))) if MODE == "virtual-drug-screen" and TASK=="ost" else [],
        expand(os.path.join(OUTPUT_DIR,"rule_AF3_INFERENCE","done_flags",f"af3_inference_jobs_{{s}}-of-{SPLIT_TOTAL}.done.txt"),s=list(range(1,SPLIT_TOTAL+1))),



checkpoint PREPROCESSING:
    input:
        INPUT_DF,
    output:
        directory(os.path.join(OUTPUT_DIR,"rule_PREPROCESSING"))
    params:
        msa_option = MSA_OPTION,
        mode = MODE,
        n_seeds = N_SEEDS,
        n_samples = N_SAMPLES,
        predict_individual_components = PREDICT_INDIVIDUAL_COMPONENTS
    shell:
        """
        python {WORKFLOW_DIR}/scripts/preprocessing.py {input[0]} {OUTPUT_DIR} --mode={params.mode} {params.predict_individual_components} --n-seeds {params.n_seeds} --n-samples {params.n_samples} 
        """



#rule AF3_DATA_SPEEDY_PIPELINE_TEST:
#    input:
#        monomers = os.path.join(OUTPUT_DIR,"rule_PREPROCESSING","monomers","{mono}.json") if MODE in ["custom", "all-vs-all","pulldown","virtual-drug-screen"]  else [],
#    output:
#        data_pipeline_monomers=touch(os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","monomers","{mono}/{mono}_data.json")) if MODE in [
#            "custom","all-vs-all", "pulldown","virtual-drug-screen"] else [],

rule AF3_DATA_SPEEDY_PIPELINE:
    input:
        monomers = os.path.join(OUTPUT_DIR,"rule_PREPROCESSING","monomers","{mono}.json") if MODE in ["custom",
                                                                                                      "stoichio-screen",
                                                                                                      "all-vs-all",
                                                                                                      "pulldown",
                                                                                                      "virtual-drug-screen"]  else [],
    params:
        mode = MODE,
        extra_af3_flags = EXTRA_AF3_FLAGS
    output:
        data_pipeline_monomers=temp(os.path.join(OUTPUT_DIR,"rule_AF3_DATA_PIPELINE","{mono}","{mono}_data.json")) if MODE in [
            "custom","stoichio-screen","all-vs-all", "pulldown","virtual-drug-screen"] else [],
    container:
        AF3_CONTAINER
    shell:
        """
        python /app/alphafold/run_alphafold.py --json_path=/root/af_output/rule_PREPROCESSING/monomers/{wildcards.mono}.json \
        --model_dir=/root/models \
        --output_dir=/root/af_output/rule_AF3_DATA_PIPELINE \
        --db_dir=/root/public_databases \
        --run_data_pipeline=true \
        --run_inference=false \
        {params.extra_af3_flags} 
        """

rule MERGE_MONO_AND_MULTI_JSON:
    input:
        data_pipeline_monomers = lambda wildcards: [os.path.join(OUTPUT_DIR, "rule_AF3_DATA_PIPELINE",f"{os.path.basename(mono).split("_data.json")[0]}",f"{os.path.basename(mono).split("_data.json")[0]}_data.json")  for mono in
                                                    get_multi_to_monomeric_dict(wildcards).loc[get_multi_to_monomeric_dict(wildcards).inference_samples.str.contains(wildcards.multi),"data_pipeline_samples"].values.tolist()]
        if MODE in ["all-vs-all", "pulldown","virtual-drug-screen", "stoichio-screen"] else [],
        preprocessed_multimers = os.path.join(OUTPUT_DIR,"rule_PREPROCESSING","multimers","{multi}.json") if MODE in ["custom","all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"]  else [],
    params:
        metdata_dir = os.path.join(OUTPUT_DIR,"rule_PREPROCESSING","metadata"),
    output:
        data_pipeline_msa = expand(os.path.join(OUTPUT_DIR,"rule_MERGE_MONOMERS_TO_MULTIMERS","{{multi}}_seed-{seed}_data.json"),seed=SEEDS) if MODE in ["custom","all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"] else [],
    shell:
        """
        python {WORKFLOW_DIR}/scripts/merge_mono_and_multi_jsons.py {params.metdata_dir}/inference_to_data_pipeline_map.tsv {params.metdata_dir}/inference_samples.tsv {wildcards.multi}
        """


rule CREATE_AF3_INFERENCE_JOBS_SPEEDY_AF3_PIPELINE:
    input:
        multimers = claude
        #multimers = os.path.join(OUTPUT_DIR,"rule_MERGE_MONOMERS_TO_MULTIMERS","{multi}_data.json") if MODE in ["custom","all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"]  else []
    params:
        mode = MODE,
        extra_af3_flags = EXTRA_AF3_FLAGS
    output:
        multimers = os.path.join(OUTPUT_DIR,"rule_CREATE_AF3_INFERENCE_JOBS","{multi}_af3_inference_job.txt") if MODE in ["custom","all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"]  else []
    shell:
        """
        echo 'python /app/alphafold/run_alphafold.py --json_path=/root/af_output/rule_MERGE_MONOMERS_TO_MULTIMERS/{wildcards.multi}_data.json \
        --model_dir=/root/models \
        --output_dir=/root/af_output/rule_AF3_INFERENCE \
        --db_dir=/root/public_databases \
        --run_data_pipeline=false \
        --run_inference=true \
        {params.extra_af3_flags}' > {output}
        """


rule AGG_AF3_INFERENCE_JOBS:
    input:
        merged_multimer = get_multimeric_json_with_msas if MODE in ["custom", "all-vs-all","pulldown","virtual-drug-screen","stoichio-screen","stoichio-screen"]  else [],
    params:
        mode = MODE
    output:
        job_list = os.path.join(OUTPUT_DIR,"rule_AGG_AF3_INFERENCE_JOBS","af3_inference_jobs.txt")
    shell:
        """
        find {OUTPUT_DIR}/rule_CREATE_AF3_INFERENCE_JOBS -type f -name '*.txt' -print0 | tar --null -T - -cf - | tar -xOf - > {output.job_list}
        """

rule SPLIT_INFERENCE_JOB_LIST:
    input:
        inference_job_list = ancient(os.path.join(OUTPUT_DIR,"rule_AGG_AF3_INFERENCE_JOBS","af3_inference_jobs.txt"))
    params:
        split_total = SPLIT_TOTAL
    output:
        sub_job_list = scatter.split(os.path.join(OUTPUT_DIR,"rule_SPLIT_INFERENCE_JOB_LIST","af3_inference_jobs_{scatteritem}.txt"))
    run:
        import os
        lines = open(input.inference_job_list).readlines()
        n_lines = len(lines)
        n_splits = params.split_total

        # Calculate chunk size
        chunk_size = n_lines // n_splits
        remainder = n_lines % n_splits

        start = 0
        for i in range(n_splits):
            # Add one extra line to the first 'remainder' splits
            end = start + chunk_size + (1 if i < remainder else 0)

            with open(output.sub_job_list[i], "w") as f:
                f.writelines(lines[start:end])

            start = end


rule AF3_INFERENCE:
    input:
        job_list = ancient(os.path.join(OUTPUT_DIR,"rule_SPLIT_INFERENCE_JOB_LIST","af3_inference_jobs_{scatteritem}.txt")),
    output:
        touch(os.path.join(OUTPUT_DIR,"rule_AF3_INFERENCE","done_flags","af3_inference_jobs_{scatteritem}.done.txt")),
    container:
        AF3_CONTAINER
    shell:
        """
        bash /scripts/parallel.sh /root/af_output/rule_SPLIT_INFERENCE_JOB_LIST/af3_inference_jobs_{wildcards.scatteritem}.txt
        """


checkpoint GET_DONE_OUTPUTS:
    input:
        expand(os.path.join(OUTPUT_DIR,"rule_AF3_INFERENCE","done_flags",f"af3_inference_jobs_{{s}}-of-{SPLIT_TOTAL}.done.txt"),s=list(range(1,SPLIT_TOTAL+1))),
    output:
        directory(os.path.join(OUTPUT_DIR,"rule_GET_DONE_OUTPUTS"))
    shell:
        """
        mkdir -p {output}
        for i in {{1..{SPLIT_TOTAL}}}; do
            eval \"$(grep -- '--json_path=' {OUTPUT_DIR}/rule_SPLIT_INFERENCE_JOB_LIST/af3_inference_jobs_${{i}}-of-{SPLIT_TOTAL}.txt  | sed -E 's/.*--json_path=\\/.*\\/([^/]+)_data\\.json.*/touch {OUTPUT_DIR}\\/rule_GET_DONE_OUTPUTS\\/\\1.done.txt/')\";
        done
        """

rule COLLECT_AF3_PREDICTIONS:
    input:
        done_files = os.path.join(OUTPUT_DIR,"rule_GET_DONE_OUTPUTS","{multi}_seed-{seed}.done.txt"),
        job = os.path.join(OUTPUT_DIR,"rule_CREATE_AF3_INFERENCE_JOBS","{multi}_seed-{seed}_af3_inference_job.txt") if MODE in ["all-vs-all","pulldown","virtual-drug-screen","stoichio-screen"]  else []
    output:
        os.path.join(OUTPUT_DIR,"COLLECT_AF3_PREDICTIONS","{multi}_seed-{seed}_sample-{sample}_model.cif")
    shell:
        """
        python {WORKFLOW_DIR}/scripts/collect_predictions.py --source-dir {OUTPUT_DIR}/rule_AF3_INFERENCE/{wildcards.multi}_seed-{wildcards.seed} --job-list {input.job} --output-dir {OUTPUT_DIR}/rule_COLLECT_AF3_PREDICTIONS
        """

rule CREATE_OST_COMAPRE_LIGAND_STRUCTURES_JOBS:
    input:
        prediction = os.path.join(OUTPUT_DIR,"COLLECT_AF3_PREDICTIONS","{multi}_seed-{seed}_sample-{sample}_model.cif") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
        ground_truth_cif=lambda wildcards: get_ground_truth_files(wildcards)["cif"] if MODE == "virtual-drug-screen" and TASK=="ost" else [],
        ground_truth_sdf=lambda wildcards: get_ground_truth_files(wildcards)["sdf"] if MODE == "virtual-drug-screen" and TASK=="ost" else []
    output:
        os.path.join(OUTPUT_DIR,"rule_CREATE_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","{multi}_seed-{seed}_sample-{sample}_job.txt") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
    shell:
        """    
        json_path={OUTPUT_DIR}/rule_OST_COMAPRE_LIGAND_STRUCTURES/{wildcards.multi}_seed-{wildcards.seed}_sample-{wildcards.sample}_score.json
        echo "ost compare-ligand-structures \
        -m {input.prediction} \
        -rl {input.ground_truth_sdf} \
        -r {input.ground_truth_cif} \
        -o $json_path \
        --lddt-pli --rmsd --lddt-pli-add-mdl-contacts -ft" > {output}
        """

rule AGG_OST_COMAPRE_LIGAND_STRUCTURES_JOBS:
    input:
        get_collect_predictions,
    output:
        os.path.join(OUTPUT_DIR,"rule_AGG_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","jobs.txt")
    shell:
        """
        cat {input} > {output}
        """

rule SPLIT_OST_COMAPRE_LIGAND_STRUCTURES_JOBS:
    input:
         rules.AGG_OST_COMAPRE_LIGAND_STRUCTURES_JOBS.output if MODE == "virtual-drug-screen" and TASK=="ost" else [],
    params:
         split_ost = SPLIT_OST
    output:
         sub_job_list = scatter.split_ost(os.path.join(OUTPUT_DIR,"rule_SPLIT_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","job_{scatteritem}.txt"))
    run:
        import os
        lines = open(input[0]).readlines()
        [open(output.sub_job_list[i], "w").writelines(lines[i*len(lines)//params.split_ost:(i+1)*len(lines)//params.split_ost + (1 if i == 3 else 0)])
         for i in range(params.split_ost)]


rule OST_COMAPRE_LIGAND_STRUCTURES:
    input:
         sub_job_list = os.path.join(OUTPUT_DIR,"rule_SPLIT_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","job_{scatteritem}.txt") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
    output:
         touch(os.path.join(OUTPUT_DIR,"rule_OST_COMAPRE_LIGAND_STRUCTURES","done_flags","job_{scatteritem}.done.txt")),
    shell:
         """
         bash {WORKFLOW_DIR}/scripts/parallel_cpu.sh {input}
         """

rule CREATE_OST_REPORT:
    input:
         sub_job_list = os.path.join(OUTPUT_DIR,"rule_SPLIT_OST_COMAPRE_LIGAND_STRUCTURES_JOBS","job_{scatteritem}.txt") if MODE == "virtual-drug-screen" and TASK=="ost" else [],
         done_file = os.path.join(OUTPUT_DIR,"rule_OST_COMAPRE_LIGAND_STRUCTURES","done_flags","job_{scatteritem}.done.txt")
    output:
         os.path.join(OUTPUT_DIR,"CREATE_OST_REPORT","{scatteritem}_ost_report.csv")
    shell:
         """
         > {wildcards.scatteritem}.commands.sh
         grep -oP '(?<=-o )\\S+' {input.sub_job_list} | while read -r path; do echo "python {WORKFLOW_DIR}/scripts/af3_analysis.py.bak $path {RUNS_N_POSES_JSON_INPUTS} {OUTPUT_DIR}/AF3_INFERENCE {OUTPUT_DIR}/OST_COMAPRE_LIGAND_STRUCTURES {RUNS_N_POSES_ANNOTATIONS} >> {output}" >> {wildcards.scatteritem}.commands.sh; done
         chmod +x {wildcards.scatteritem}.commands.sh
         bash {WORKFLOW_DIR}/scripts/parallel_cpu.sh {wildcards.scatteritem}.commands.sh
         rm {wildcards.scatteritem}.commands.sh
         """

rule AGG_OST_REPORTS:
    input:
         expand(os.path.join(OUTPUT_DIR,"CREATE_OST_REPORT",f"{{s}}-of-{SPLIT_OST}_ost_report.csv"),s=list(range(1,SPLIT_OST+1))) if MODE == "virtual-drug-screen" and TASK=="ost" else [],
    output:
         os.path.join(OUTPUT_DIR,"rule_AGG_OST_REPORTS","ost_report.csv")
    shell:
        """
        > {output}
        echo "target method seed sample ranking_score prot_lig_chain_iptm_average_lddt_pli prot_lig_chain_iptm_min_lddt_pli prot_lig_chain_iptm_max_lddt_pli lig_prot_chain_iptm_average_lddt_pli lig_prot_chain_iptm_min_lddt_pli lig_prot_chain_iptm_max_lddt_pli lddt_pli model_ligand_chain_lddt_pli model_ligand_ccd_code model_ligand_smiles ligand_ccd_code_x prot_lig_chain_iptm_average_rmsd prot_lig_chain_iptm_min_rmsd prot_lig_chain_iptm_max_rmsd lig_prot_chain_iptm_average_rmsd lig_prot_chain_iptm_min_rmsd lig_prot_chain_iptm_max_rmsd rmsd lddt_lp bb_rmsd model_ligand_chain_rmsd ligand_ccd_code_y ligand_instance_chain ligand_is_proper" > {output}
        cat {input} >> {output}
        """
